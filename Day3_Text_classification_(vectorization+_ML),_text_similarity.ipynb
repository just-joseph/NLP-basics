{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day3- Text classification (vectorization+ ML), text similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOq9Af6rhILzrSd/ZSefMpJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/just-joseph/NLP-basics/blob/main/Day3_Text_classification_(vectorization%2B_ML)%2C_text_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqs8x09LjYZa"
      },
      "source": [
        "### Cosine Similarity using tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbCH4dlZrRFU"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxdjsdQkjbWk",
        "outputId": "1d67722e-746c-4323-cc45-b0b75e03991c"
      },
      "source": [
        "#documents\n",
        "corpus= [\"I work as software engineer.\", \"I am a cinephile.\", \"I love treks.\"]\n",
        "tfidfvectorizer = TfidfVectorizer()\n",
        "tfidfvectorizer.fit(corpus)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XXsFMGeop0F",
        "outputId": "da9220b5-9b21-4091-e09e-5b0409270e92"
      },
      "source": [
        "tfidf_test_vectors= tfidfvectorizer.transform( corpus ).toarray()\n",
        "tfidf_query_vector= tfidfvectorizer.transform( ['I love to build and engineer software.'] ).toarray()\n",
        "print(tfidf_query_vector)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.57735027 0.57735027 0.57735027\n",
            "  0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKRNf7E3ovcy",
        "outputId": "03db0cb5-3318-4b08-d9ee-1e2546cf265b"
      },
      "source": [
        "print(cosine_similarity( tfidf_test_vectors, tfidf_query_vector ) )"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.57735027]\n",
            " [0.        ]\n",
            " [0.40824829]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opHVvFu2lQFK"
      },
      "source": [
        "tfidf_test_vectors= tfidfvectorizer.transform( corpus ).toarray()\n",
        "tfidf_query_vector= tfidfvectorizer.transform( ['My profession is related to computers'] ).toarray()"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6yvJodtlcKc",
        "outputId": "72e7b011-2cae-4d8f-a8fc-47f5102689da"
      },
      "source": [
        "print(cosine_similarity( tfidf_test_vectors, tfidf_query_vector ) )"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdbwhOzrpaTg"
      },
      "source": [
        "### Cosine Similarity using Spacy (using pretrained spacy sm model for English)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qju8ofRZpU3x",
        "outputId": "351a6675-7812-420e-d239-9ac245c40d7d"
      },
      "source": [
        "!pip install spacy\n",
        "!pip install en-core-web-sm"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.1.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: en-core-web-sm in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (56.1.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en-core-web-sm) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en-core-web-sm) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en-core-web-sm) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbwFnTkypjwN"
      },
      "source": [
        "import spacy\n",
        "nlp= spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX8gRXCPpnoy"
      },
      "source": [
        "sentences= [\"I work as a software engineer.\", \"I am a cinephile.\", \"I love treks.\"]\n",
        "query= ['My profession is related to computers']\n",
        "\n",
        "sentence_vectors=[]\n",
        "for doc in nlp.pipe(sentences):\n",
        "    sentence_vectors.append(doc.vector)\n",
        "clause_vector=[]\n",
        "for doc in nlp.pipe( query ):\n",
        "    clause_vector.append(doc.vector)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vauqxek0qMKE",
        "outputId": "f7b4f50e-0647-429c-a3f5-a2c84d7488ce"
      },
      "source": [
        "sentence_vectors[0]"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.42306647,  0.410245  , -0.21554174, -1.0574677 ,  0.56674945,\n",
              "       -0.65784615,  0.71261966,  1.3172448 ,  1.5559533 ,  0.9395925 ,\n",
              "       -0.6180364 , -0.00714145, -0.47274384, -0.3331472 , -0.29177096,\n",
              "       -1.5721703 , -0.8411566 , -0.9079747 , -0.3509024 , -0.04010289,\n",
              "        0.48536736,  0.83592623,  0.784914  , -0.04399982, -2.3147044 ,\n",
              "        0.5319873 , -0.44756716, -0.8045259 ,  0.93263996, -0.8893102 ,\n",
              "        1.0646421 ,  1.2814503 ,  0.9157917 ,  0.14049768,  0.18912308,\n",
              "       -2.1003816 , -0.04817145, -1.0376045 , -0.03952559, -0.4662034 ,\n",
              "        1.6607322 , -0.49522597,  0.97951376, -1.7778645 ,  0.9311367 ,\n",
              "       -1.3178028 ,  0.5249395 ,  0.76417106, -2.3682735 ,  1.2034246 ,\n",
              "        0.36784664, -1.8730861 ,  0.6806672 ,  0.9046923 , -1.712573  ,\n",
              "        0.54825383,  1.4249355 , -0.29313308, -0.24200244,  1.0566458 ,\n",
              "        0.41982612,  0.21106339,  1.8621495 ,  0.66706425, -0.29118863,\n",
              "       -0.06374728, -0.22001018, -1.770602  , -0.7503467 , -0.07544237,\n",
              "       -0.4150067 ,  0.71739346, -1.409197  , -0.62669843, -0.37381268,\n",
              "        0.10945696,  2.0793674 , -1.1327431 ,  0.10479331, -1.0443553 ,\n",
              "        0.7430879 , -0.5657424 ,  0.7175654 ,  0.11678771,  0.7127682 ,\n",
              "       -0.31381273,  0.6199767 ,  1.258936  , -0.7602283 ,  0.7840515 ,\n",
              "        0.08727553, -0.65922135,  0.22950427,  0.8074537 ,  1.3530815 ,\n",
              "        0.7695686 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeSHHv2up9fU",
        "outputId": "2c83b823-81db-49cb-e4f7-9b77e0635d3d"
      },
      "source": [
        "print(cosine_similarity( sentence_vectors, clause_vector ) )"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.5140746 ]\n",
            " [0.35023063]\n",
            " [0.25728053]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Zt8wmrrKfQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFvLSfVhrKhw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtbloTu6rKkM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzVegHgrKmg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pos2wH7yWuTL",
        "outputId": "a6f5ba43-2348-46f2-ca27-b7bb403c55f7"
      },
      "source": [
        "## importing all libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Case 1 : Would use Word2Vec and then apply different classifiers\n",
        "from gensim.models import Word2Vec  \n",
        "\n",
        "## Case 2 : Would use TfidfVectorizer and then apply different classifiers\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer   \n",
        "\n",
        "## This would help us understand : Does the type of Vectorizing Process influence the precision , accuracy , recall score of the classifiers ??\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "ISlbi9TSWvFs",
        "outputId": "a29b7720-b646-4589-f05e-30dfc6b082a9"
      },
      "source": [
        "# Read data frame\n",
        "file_url = 'https://raw.githubusercontent.com/just-joseph/NLP-basics/main/bbc-text.csv'\n",
        "df = pd.read_csv(file_url)\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "SrpTopM4X-QF",
        "outputId": "1ed2e4e3-8dc2-4c95-b41b-9c775cfe915f"
      },
      "source": [
        "## Doing basic EDA ( Exploratory Data Analysis )\n",
        "\n",
        "# Description of the dataset\n",
        "print('SHAPE OF DATASET: ', df.shape, '\\n\\nCOLUMNS IN DATASET: ', df.columns, '\\n\\nCATEGORIES: ', df.category.unique(), '\\n\\n')\n",
        "\n",
        "# Plotting number of samples within each category\n",
        "print('NUMBER OF SAMPLES IN EACH CATEGORY: \\n')\n",
        "sns.countplot(df.category)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHAPE OF DATASET:  (2225, 2) \n",
            "\n",
            "COLUMNS IN DATASET:  Index(['category', 'text'], dtype='object') \n",
            "\n",
            "CATEGORIES:  ['tech' 'business' 'sport' 'entertainment' 'politics'] \n",
            "\n",
            "\n",
            "NUMBER OF SAMPLES IN EACH CATEGORY: \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f17a2145e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcUH8gyKXJxB"
      },
      "source": [
        "Note : Data set seems balanced. Just in case if it was highly imbalanced say any one category had more than 50% entries then , we could have used SMOTHE (Synthetic Minority Over-sampling Technique) over the Minor classes and undersampling of Major class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgkZU0zrhvMt",
        "outputId": "cb3774a2-0b4f-412c-da46-e5992532c9af"
      },
      "source": [
        "df['category']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                tech\n",
              "1            business\n",
              "2               sport\n",
              "3               sport\n",
              "4       entertainment\n",
              "            ...      \n",
              "2220         business\n",
              "2221         politics\n",
              "2222    entertainment\n",
              "2223         politics\n",
              "2224            sport\n",
              "Name: category, Length: 2225, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9_qqJuJXGjE",
        "outputId": "0750c737-ab0c-4874-ad2f-b63772f4fbf2"
      },
      "source": [
        "# DATA CLEANING\n",
        "print('Data cleaning in progress...')\n",
        "\n",
        "# Tokenize : dividing Sentences into words\n",
        "df['text_clean'] = df['text'].apply(nltk.word_tokenize)\n",
        "print('Tokenization complete.')\n",
        "\n",
        "\n",
        "# Remove stop words\n",
        "stop_words=set(nltk.corpus.stopwords.words(\"english\"))\n",
        "df['text_clean'] = df['text_clean'].apply(lambda x: [item for item in x if item not in stop_words])\n",
        "print('Stop words removed.')\n",
        "\n",
        "\n",
        "# Remove numbers, punctuation and special characters (only keep words)\n",
        "regex = '[a-z]+'\n",
        "df['text_clean'] = df['text_clean'].apply(lambda x: [item for item in x if re.match(regex, item)])\n",
        "print('Numbers, punctuation and special characters removed.')\n",
        "\n",
        "\n",
        "# Lemmatization : lemma means base form of a word.  // Example : leaf and leaves get lemmatized to leaf\n",
        "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "df['text_clean'] = df['text_clean'].apply(lambda x: [lem.lemmatize(item, pos='v') for item in x])\n",
        "print('Lemmatization complete.\\nData cleaning complete.\\n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data cleaning in progress...\n",
            "Tokenization complete.\n",
            "Stop words removed.\n",
            "Numbers, punctuation and special characters removed.\n",
            "Lemmatization complete.\n",
            "Data cleaning complete.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tviN4TZXV8e",
        "outputId": "5ff15e69-0d89-4695-95e8-74c3a67bdaad"
      },
      "source": [
        "# Classification using word2vec vectorizer\n",
        "\n",
        "vec_model = Word2Vec(df['text_clean'])\n",
        "\n",
        "w2v = dict(zip(vec_model.wv.index2word, vec_model.wv.syn0))\n",
        "## What is syn0 ?\n",
        "## https://stackoverflow.com/questions/53301916/python-gensim-what-is-the-meaning-of-syn0-and-syn0norm\n",
        "\n",
        "class Vectorizer(object):\n",
        "    \n",
        "    def __init__(self, vec):\n",
        "        self.vec = vec\n",
        "        self.dim = len(vec.values())\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([np.mean([self.vec[w] for w in words if w in self.vec] or [np.zeros(self.dim)], axis=0) for words in X])\n",
        "\n",
        "\n",
        "## for any Classifier , we need intialise the model with the parameters. \n",
        "## Further I am applying GridSearchCV for 5 runs (i.e 1/5th data used each time for testing) \n",
        "## So the model gets trained over 5 runs \n",
        "## as well we are predicting also over 5 runs\n",
        "## In case if you wish to know about Cross Validation , you can watch the Video here : https://www.youtube.com/watch?v=LmxsySwAhoE&t=84s\n",
        "class Classifier(object):\n",
        "    \n",
        "    def __init__(self, model, param):\n",
        "        self.model = model\n",
        "        self.param = param\n",
        "        self.gs = GridSearchCV(self.model, self.param, cv=5, error_score=0, refit=True)        \n",
        "\n",
        "    def fit(self, X, y):        \n",
        "        return self.gs.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.gs.predict(X)\n",
        "    \n",
        "\n",
        "## Preparing to make a pipeline \n",
        "## What to know about Pipelining : see this https://www.youtube.com/watch?v=Y4iJfKX_QeQ&t=52s\n",
        "clf_models = {\n",
        "    'Naive Bayes': GaussianNB(), \n",
        "    # 'SVC': SVC(),\n",
        "    # 'Decision Tree': DecisionTreeClassifier(),  \n",
        "    # 'Perceptron': MLPClassifier(),\n",
        "    # 'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "clf_params = {\n",
        "    'Naive Bayes': { }, \n",
        "    # 'SVC': { 'kernel': ['linear', 'rbf'] },\n",
        "    # 'Decision Tree': { 'min_samples_split': [2, 5] }, \n",
        "    # 'Perceptron': { 'activation': ['tanh', 'relu'] },\n",
        "    # 'Gradient Boosting': { 'min_samples_split': [2, 5] }\n",
        "}\n",
        "\n",
        "\n",
        "## splitting the dataset into 80:20.  have kept shuffle=True , so that the data is randomly sampled or simply said shuffled , and then split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size=0.2, shuffle=True)\n",
        "\n",
        "## for loop traverses , each and every classifier and its corresponding parameters.\n",
        "for key in clf_models.keys():\n",
        "    \n",
        "    clf = Pipeline([('Word2Vec vectorizer', Vectorizer(w2v)), ('Classifier', Classifier(clf_models[key], clf_params[key]))])\n",
        "    \n",
        "    clf.fit(X_train, y_train)  ## Note : we are calling user defined fit method. This fit method uses Cross Validation\n",
        "    y_pred = clf.predict(X_test)  ## Note : we are calling user defined predict method. This predict method uses Cross Validation\n",
        "    \n",
        "    ## printing performance metrics for each classifier \n",
        "    print(key, ':')\n",
        "    print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes :\n",
            "Accuracy: 0.346 \tPrecision: 0.341 \tRecall: 0.332 \t\tF1: 0.316\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdUoxYbYYIOB",
        "outputId": "ed042499-98d9-4174-9114-75646fed5470"
      },
      "source": [
        "# Classification using TFIDF vectorizer\n",
        "\n",
        "# Vectorize training and testing data. Here we would pass TfidfVectorizer() to vec \n",
        "def Vectorize(vec, X_train, X_test):    \n",
        "    \n",
        "    X_train_vec = vec.fit_transform(X_train)\n",
        "    X_test_vec = vec.transform(X_test)\n",
        "    \n",
        "    print('Vectorization complete.\\n')\n",
        "    \n",
        "    return X_train_vec, X_test_vec\n",
        "\n",
        "\n",
        "# Use multiple classifiers and grid search for prediction\n",
        "def ML_modeling(models, params, X_train, X_test, y_train, y_test):    \n",
        "    \n",
        "    if not set(models.keys()).issubset(set(params.keys())):\n",
        "        raise ValueError('Some estimators are missing parameters')\n",
        "\n",
        "    for key in models.keys():\n",
        "    \n",
        "        model = models[key]\n",
        "        param = params[key]\n",
        "        gs = GridSearchCV(model, param, cv=5, error_score=0, refit=True)\n",
        "        gs.fit(X_train, y_train)\n",
        "        y_pred = gs.predict(X_test)\n",
        "        \n",
        "        # Print scores for the classifier\n",
        "        print(key, ':', gs.best_params_)\n",
        "        print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))\n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "## Preparing to make a pipeline \n",
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(), \n",
        "    # 'Decision Tree': DecisionTreeClassifier(),  \n",
        "    # 'Perceptron': MLPClassifier(),\n",
        "    # 'Gradient Boosting': GradientBoostingClassifier()   ## This model would take a little longer to run \n",
        "}\n",
        "\n",
        "params = {\n",
        "    'Naive Bayes': { 'alpha': [0.5, 1], 'fit_prior': [True, False] }, \n",
        "    # 'Decision Tree': { 'min_samples_split': [1, 2, 5] }, \n",
        "    # 'Perceptron': { 'alpha': [0.0001, 0.001], 'activation': ['tanh', 'relu'] },\n",
        "    # 'Gradient Boosting': { 'learning_rate': [0.05, 0.1], 'min_samples_split': [2, 5] }\n",
        "}\n",
        "\n",
        "\n",
        "# Encode label categories to numbers\n",
        "enc = LabelEncoder()\n",
        "df['category'] = enc.fit_transform(df['category'])\n",
        "labels = list(enc.classes_)\n",
        "\n",
        "# Train-test split and vectorize\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size=0.2, shuffle=True)\n",
        "X_train_vec, X_test_vec = Vectorize(TfidfVectorizer(), X_train, X_test)\n",
        "\n",
        "ML_modeling(models, params, X_train_vec, X_test_vec, y_train, y_test)\n",
        "## ML_modeling method also prints performance scores for each classifier\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization complete.\n",
            "\n",
            "Naive Bayes : {'alpha': 0.5, 'fit_prior': False}\n",
            "Accuracy: 0.964 \tPrecision: 0.965 \tRecall: 0.963 \t\tF1: 0.963\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spCXEQL2Yp9f"
      },
      "source": [
        "Conclusion of this Ananlysis : TfidfVectorizer seems to have performed far better than Word2Vec vectorizer. So with this simple excerise , its proved that we should prefer to use TfidfVectorizer for most Text (NLP) applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1LiWmV6YrVq"
      },
      "source": [
        "Tfidf is useful when the embedding/ vectorization is created using our own corpus. But if we have access to huge corpuses of data, Word2vec outperforms Tfidf. Better, if we have pre-trained Word2vec embeddings (pre-trained on huge data), go for Word2vec than tfidf as the former would have better English language understanding."
      ]
    }
  ]
}