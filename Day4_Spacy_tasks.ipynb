{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPT/AR6l1XMzbChl5M3Se0P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/just-joseph/NLP-basics/blob/main/Day4_Spacy_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPj8opb8u8dq"
      },
      "source": [
        "spaCy is an advanced modern library for Natural Language Processing developed by Matthew Honnibal and Ines Montani. It is designed to be industrial grade but open source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iTn_m4yk3ku",
        "outputId": "722fc295-00ab-4720-a225-ef1a2c1a2e94"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.1.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6wG42iau-wC"
      },
      "source": [
        "spaCy comes with pretrained NLP models that can perform most common NLP tasks, such as tokenization, parts of speech (POS) tagging, named entity recognition (NER), lemmatization, transforming to word vectors etc.\n",
        "\n",
        "If you are dealing with a particular language, you can load the spacy model specific to the language using spacy.load() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lyFSlv1uqxt"
      },
      "source": [
        "import spacy\n",
        "# Load small english model: https://spacy.io/models\n",
        "nlp= spacy.load(\"en_core_web_sm\")  # most other models need to be installed before we use"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74EMStQavE4_"
      },
      "source": [
        "This returns a Language object that comes ready with multiple built-in capabilities.\n",
        "\n",
        "Now, let us say you have your text data in a string. What can be done to understand the structure of the text?\n",
        "\n",
        "First, call the loaded nlp object on the text. It should return a processed Doc object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3XbiuukuuLG",
        "outputId": "b2c53931-9591-4527-bb65-e7ec341bd2f6"
      },
      "source": [
        "my_text = \"\"\"The economic situation of the country is on edge , as the stock \n",
        "market crashed causing loss of millions. Citizens who had their main investment \n",
        "in the share-market are facing a great loss. Many companies might lay off \n",
        "thousands of people to reduce labor cost\"\"\"\n",
        "\n",
        "my_doc = nlp(my_text)\n",
        "type(my_doc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQlgidLkvPZy"
      },
      "source": [
        "But, what exactly is a Doc object ?\n",
        "\n",
        "It is a sequence of tokens that contains not just the original text but all the results produced by the spaCy model after processing the text. Useful information such as the lemma of the text, whether it is a stop word or not, named entities, the word vector of the text and so on are pre-computed and readily stored in the Doc object.\n",
        "\n",
        "The good thing is that you have complete control on what information needs to be pre-computed and customized. We will see all of that shortly.\n",
        "\n",
        "Also, though the text gets split into tokens, no information of the original text is actually lost.\n",
        "\n",
        "What is a Token?\n",
        "\n",
        "Tokens are individual text entities that make up the text. Typically a token can be the words, punctuation, spaces, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmcLQ123vTXN"
      },
      "source": [
        "**Tokenization** is the process of converting a text into smaller sub-texts, based on certain predefined rules. For example, sentences are tokenized to words (and punctuation optionally). And paragraphs into sentences, depending on the context.\n",
        "\n",
        "This is typically the first step for NLP tasks like text classification, sentiment analysis, etc.\n",
        "\n",
        "Each token in spacy has different attributes that tell us a great deal of information.\n",
        "\n",
        "Such as, if the token is a punctuation, what part-of-speech (POS) is it, what is the lemma of the word etc. This article will cover everything from A-Z.\n",
        "\n",
        "Let’s see the token texts on my_doc. The string which the token represents can be accessed through the token.text attribute.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj-ev0HfuxNB",
        "outputId": "2f9f6b4c-21b9-4bea-9f73-8009a4425459"
      },
      "source": [
        "# Printing the tokens of a doc\n",
        "for token in my_doc:\n",
        "  print(token.text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The\n",
            "economic\n",
            "situation\n",
            "of\n",
            "the\n",
            "country\n",
            "is\n",
            "on\n",
            "edge\n",
            ",\n",
            "as\n",
            "the\n",
            "stock\n",
            "\n",
            "\n",
            "market\n",
            "crashed\n",
            "causing\n",
            "loss\n",
            "of\n",
            "millions\n",
            ".\n",
            "Citizens\n",
            "who\n",
            "had\n",
            "their\n",
            "main\n",
            "investment\n",
            "\n",
            "\n",
            "in\n",
            "the\n",
            "share\n",
            "-\n",
            "market\n",
            "are\n",
            "facing\n",
            "a\n",
            "great\n",
            "loss\n",
            ".\n",
            "Many\n",
            "companies\n",
            "might\n",
            "lay\n",
            "off\n",
            "\n",
            "\n",
            "thousands\n",
            "of\n",
            "people\n",
            "to\n",
            "reduce\n",
            "labor\n",
            "cost\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n02Prrr8vbgr"
      },
      "source": [
        "The above tokens contain punctuation and common words like “a”, ” the”, “was”, etc. These do not add any value to the meaning of your text. They are called stop words.\n",
        "\n",
        "Let’s clean it up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkVmYSNtvhJF"
      },
      "source": [
        "As mentioned in the last section, there is ‘noise’ in the tokens. The words such as ‘the’, ‘was’, ‘it’ etc are very common and are referred as ‘stop words’.\n",
        "\n",
        "Besides, you have punctuation like commas, brackets, full stop and some extra white spaces too. The process of removing noise from the doc is called Text Cleaning or Preprocessing.\n",
        "\n",
        "What is the need for Text Preprocessing ?\n",
        "\n",
        "The outcome of the NLP task you perform, be it classification, finding sentiments, topic modeling etc, the quality of the output depends heavily on the quality of the input text used.\n",
        "\n",
        "Stop words and punctuation usually (not always) don’t add value to the meaning of the text and can potentially impact the outcome. To avoid this, its might make sense to remove them and clean the text of unwanted characters can reduce the size of the corpus.\n",
        "\n",
        "How to identify and remove the stopwords and punctuation?\n",
        "\n",
        "The tokens in spacy have attributes which will help you identify if it is a stop word or not.\n",
        "\n",
        "The token.is_stop attribute tells you that. Likewise, token.is_punct and token.is_space tell you if a token is a punctuation and white space respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwOVSbhpvYmn",
        "outputId": "5934b2f9-896c-41ed-fb60-af027297aa8f"
      },
      "source": [
        "# Printing tokens and boolean values stored in different attributes\n",
        "for token in my_doc:\n",
        "  print(token.text,'--',token.is_stop,'---',token.is_punct)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The -- True --- False\n",
            "economic -- False --- False\n",
            "situation -- False --- False\n",
            "of -- True --- False\n",
            "the -- True --- False\n",
            "country -- False --- False\n",
            "is -- True --- False\n",
            "on -- True --- False\n",
            "edge -- False --- False\n",
            ", -- False --- True\n",
            "as -- True --- False\n",
            "the -- True --- False\n",
            "stock -- False --- False\n",
            "\n",
            " -- False --- False\n",
            "market -- False --- False\n",
            "crashed -- False --- False\n",
            "causing -- False --- False\n",
            "loss -- False --- False\n",
            "of -- True --- False\n",
            "millions -- False --- False\n",
            ". -- False --- True\n",
            "Citizens -- False --- False\n",
            "who -- True --- False\n",
            "had -- True --- False\n",
            "their -- True --- False\n",
            "main -- False --- False\n",
            "investment -- False --- False\n",
            "\n",
            " -- False --- False\n",
            "in -- True --- False\n",
            "the -- True --- False\n",
            "share -- False --- False\n",
            "- -- False --- True\n",
            "market -- False --- False\n",
            "are -- True --- False\n",
            "facing -- False --- False\n",
            "a -- True --- False\n",
            "great -- False --- False\n",
            "loss -- False --- False\n",
            ". -- False --- True\n",
            "Many -- True --- False\n",
            "companies -- False --- False\n",
            "might -- True --- False\n",
            "lay -- False --- False\n",
            "off -- True --- False\n",
            "\n",
            " -- False --- False\n",
            "thousands -- False --- False\n",
            "of -- True --- False\n",
            "people -- False --- False\n",
            "to -- True --- False\n",
            "reduce -- False --- False\n",
            "labor -- False --- False\n",
            "cost -- False --- False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty3rfbR7vkxL",
        "outputId": "588b7c70-ac91-4237-a688-549dd3e03545"
      },
      "source": [
        "# Removing StopWords and punctuations\n",
        "my_doc_cleaned = [token for token in my_doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "for token in my_doc_cleaned:\n",
        "  print(token.text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "economic\n",
            "situation\n",
            "country\n",
            "edge\n",
            "stock\n",
            "\n",
            "\n",
            "market\n",
            "crashed\n",
            "causing\n",
            "loss\n",
            "millions\n",
            "Citizens\n",
            "main\n",
            "investment\n",
            "\n",
            "\n",
            "share\n",
            "market\n",
            "facing\n",
            "great\n",
            "loss\n",
            "companies\n",
            "lay\n",
            "\n",
            "\n",
            "thousands\n",
            "people\n",
            "reduce\n",
            "labor\n",
            "cost\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HIDz-SKvtY9"
      },
      "source": [
        "You can now see that the cleaned doc has only tokens that contribute to meaning in some way.\n",
        "\n",
        "Also , the computational costs decreases by a great amount due to reduce in the number of tokens. In order to grasp the effect of Preprocessing on large text data , you can excecute the below code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f62vxMoFvpWi",
        "outputId": "0175dd1b-bbc3-489b-b4ff-359c481ecef6"
      },
      "source": [
        "# Lemmatizing the tokens of a doc\n",
        "text='she played chess against rita she likes playing chess.'\n",
        "doc=nlp(text)\n",
        "for token in doc:\n",
        "  print(token.lemma_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-PRON-\n",
            "play\n",
            "chess\n",
            "against\n",
            "rita\n",
            "-PRON-\n",
            "like\n",
            "play\n",
            "chess\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eofaa5zJwHP7"
      },
      "source": [
        "Recall that we used is_punct and is_space attributes in Text Preprocessing. They are called as **‘lexical attributes’.**\n",
        "\n",
        "In this section, you will learn about a few more significant lexical attributes.\n",
        "\n",
        "The spaCy model provides many useful lexical attributes. These are the attributes of Token object, that give you information on the type of token.\n",
        "\n",
        "For example, you can use like_num or like_email (get emails) attribute of a token to check if it is a number. Let’s print all the numbers in a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDDxx_pNvxpD",
        "outputId": "3fea330d-ee01-4ae4-bc3d-9da067d4cc58"
      },
      "source": [
        "# Printing the tokens which are like numbers\n",
        "text=' 2020 is far worse than 2009'\n",
        "doc=nlp(text)\n",
        "for token in doc:\n",
        "  if token.like_num:\n",
        "    print(token)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020\n",
            "2009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHcxG6M3wWP3"
      },
      "source": [
        "Likewise, spaCy provides a variety of token attributes. Below is a list of those attributes and the function they perform.\n",
        "1. token.is_alpha : Returns True if the token is an alphabet\n",
        "2. token.is_ascii : Returns True if the token belongs to ascii characters\n",
        "3. token.is_digit : Returns True if the token is a number(0-9)\n",
        "4. token.is_upper : Returns True if the token is upper case alphabet\n",
        "5. token.is_lower : Returns True if the token is lower case alphabet\n",
        "6. token.is_space : Returns True if the token is a space ‘ ‘\n",
        "7. token.is_bracket : Returns True if the token is a bracket\n",
        "8. token.is_quote : Returns True if the token is a quotation mark\n",
        "9. token.like_url : Returns True if the token is similar to a URl (link to website)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqvwxWOjwirK"
      },
      "source": [
        "**Part of Speech analysis with spaCy**\n",
        "Consider a sentence , “Emily likes playing football”.\n",
        "\n",
        "Here , Emily is a NOUN , and playing is a VERB. Likewise , each word of a text is either a noun, pronoun, verb, conjection, etc. These tags are called as Part of Speech tags (POS).\n",
        "\n",
        "How to identify the part of speech of the words in a text document ?\n",
        "\n",
        "It is present in the pos_ attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-id9ObJwLM9",
        "outputId": "9e264748-ce41-4d4a-a715-cd8abccc460f"
      },
      "source": [
        "# POS tagging using spaCy\n",
        "my_text='John plays basketball,if time permits. He played in high school too.'\n",
        "my_doc=nlp(my_text)\n",
        "for token in my_doc:\n",
        "  print(token.text,'---- ',token.pos_)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "John ----  PROPN\n",
            "plays ----  VERB\n",
            "basketball ----  NOUN\n",
            ", ----  PUNCT\n",
            "if ----  SCONJ\n",
            "time ----  NOUN\n",
            "permits ----  VERB\n",
            ". ----  PUNCT\n",
            "He ----  PRON\n",
            "played ----  VERB\n",
            "in ----  ADP\n",
            "high ----  ADJ\n",
            "school ----  NOUN\n",
            "too ----  ADV\n",
            ". ----  PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJxvhWRiwqvk"
      },
      "source": [
        "From above output , you can see the POS tag against each word like VERB , ADJ, etc..\n",
        "\n",
        "What if you don’t know what the tag SCONJ means ?\n",
        "\n",
        "Using spacy.explain() function , you can know the explanation or full-form in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S1jLBhMvwmQo",
        "outputId": "350067c9-1dac-40c1-8d64-b434813a74a0"
      },
      "source": [
        "spacy.explain('SCONJ')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'subordinating conjunction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57DSwpTYwx6S"
      },
      "source": [
        "**Example application of POS tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7Ol_Cdiws8V",
        "outputId": "bd873f8b-57ca-46ea-e5b3-82438e646f63"
      },
      "source": [
        "# Raw text document\n",
        "raw_text=\"\"\"I liked the movies etc The movie had good direction  The movie was amazing i.e.\n",
        "            The movie was average direction was not bad The cinematography was nice. i.e.\n",
        "            The movie was a bit lengthy  otherwise fantastic  etc etc\"\"\"\n",
        "\n",
        "# Creating a spacy object\n",
        "raw_doc=nlp(raw_text)\n",
        "\n",
        "# Checking if POS tag is X and printing them\n",
        "print('The junk values are..')\n",
        "for token in raw_doc:\n",
        "  if token.pos_=='X':\n",
        "    print(token.text)\n",
        "\n",
        "print('After removing junk')\n",
        "# Removing the tokens whose POS tag is junk.\n",
        "clean_doc=[token for token in raw_doc if not token.pos_=='X']\n",
        "print(clean_doc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The junk values are..\n",
            "etc\n",
            "i.e.\n",
            "i.e.\n",
            "etc\n",
            "etc\n",
            "After removing junk\n",
            "[I, liked, the, movies, The, movie, had, good, direction,  , The, movie, was, amazing, \n",
            "            , The, movie, was, average, direction, was, not, bad, The, cinematography, was, nice, ., \n",
            "            , The, movie, was, a, bit, lengthy,  , otherwise, fantastic,  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt_KsvOtw0ki",
        "outputId": "4b07743b-65c8-42d2-dc4b-afd5ed0b1987"
      },
      "source": [
        "#You can also know what types of tokens are present in your text by creating a dictionary shown below.\n",
        "# creating a dictionary with parts of speeach &amp; corresponding token numbers.\n",
        "\n",
        "all_tags = {token.pos: token.pos_ for token in raw_doc}\n",
        "print(all_tags)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{95: 'PRON', 100: 'VERB', 90: 'DET', 92: 'NOUN', 101: 'X', 87: 'AUX', 84: 'ADJ', 103: 'SPACE', 94: 'PART', 97: 'PUNCT', 86: 'ADV'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "028HWJyLw79b",
        "outputId": "28b046e3-818b-4f1e-e317-40b0e932e999"
      },
      "source": [
        "# For better understanding of various POS of a sentence, you can use the visualization function displacy of spacy.\n",
        "# Importing displacy\n",
        "from spacy import displacy\n",
        "my_text='She never likes playing , reading was her hobby'\n",
        "my_doc=nlp(my_text)\n",
        "\n",
        "# displaying tokens with their POS tags\n",
        "displacy.render(my_doc,style='dep',jupyter=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9deaa85d47da4693acfbf34cd62a5429-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">She</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">never</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">likes</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">playing ,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">reading</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">was</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">her</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">hobby</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9deaa85d47da4693acfbf34cd62a5429-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9deaa85d47da4693acfbf34cd62a5429-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9deaa85d47da4693acfbf34cd62a5429-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9deaa85d47da4693acfbf34cd62a5429-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9deaa85d47da4693acfbf34cd62a5429-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9deaa85d47da4693acfbf34cd62a5429-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9deaa85d47da4693acfbf34cd62a5429-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9deaa85d47da4693acfbf34cd62a5429-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9deaa85d47da4693acfbf34cd62a5429-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9deaa85d47da4693acfbf34cd62a5429-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9deaa85d47da4693acfbf34cd62a5429-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9deaa85d47da4693acfbf34cd62a5429-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9deaa85d47da4693acfbf34cd62a5429-0-6\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9deaa85d47da4693acfbf34cd62a5429-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMrUpXAyxNY9"
      },
      "source": [
        "Have a look at this text “John works at Google″. In this, ” John ” and ” Google ” are names of a person and a company. These words are referred as named-entities. They are real-world objects like name of a company , place,etc..\n",
        "\n",
        "How can find all the named-entities in a text ?\n",
        "\n",
        "Using spaCy’s ents attribute on a document, you can access all the named-entities present in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr2otpXtxH1G",
        "outputId": "f776abe0-77da-4f8f-daac-54966ff9cf8d"
      },
      "source": [
        "# Preparing the spaCy document\n",
        "text='Tony Stark owns the company StarkEnterprises . Emily Clark works at Microsoft and lives in Manchester. She loves to read the Bible and learn French'\n",
        "doc=nlp(text)\n",
        "\n",
        "# Printing the named entities\n",
        "print(doc.ents)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Tony Stark, StarkEnterprises, Emily Clark, Microsoft, Manchester, Bible, French)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBelsHMUxSK7"
      },
      "source": [
        "You can see all the named entities printed.\n",
        "\n",
        "But , is this complete information ? NO.\n",
        "\n",
        "Each named entity belongs to a category, like name of a person, or an organization, or a city, etc. The common Named Entity categories supported by spacy are :\n",
        "\n",
        "PERSON : Denotes names of people\n",
        "GPE : Denotes places like counties, cities, states.\n",
        "ORG : Denotes organizations or companies\n",
        "WORK_OF_ART : Denotes titles of books, fimls,songs and other arts\n",
        "PRODUCT : Denotes products such as vehicles, food items ,furniture and so on.\n",
        "EVENT : Denotes historical events like wars, disasters ,etc…\n",
        "LANGUAGE : All the recognized languages across the globe.\n",
        "How can you find out which named entity category does a given text belong to?\n",
        "\n",
        "You can access the same through .label_ attribute of spacy. It prints the label of named entities as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9-IGAIxxPHJ",
        "outputId": "8e02fd63-ddff-4319-ed30-1ec0aac429e8"
      },
      "source": [
        "# Printing labels of entities.\n",
        "for entity in doc.ents:\n",
        "  print(entity.text,'--- ',entity.label_)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tony Stark ---  PERSON\n",
            "StarkEnterprises ---  ORG\n",
            "Emily Clark ---  PERSON\n",
            "Microsoft ---  ORG\n",
            "Manchester ---  GPE\n",
            "Bible ---  WORK_OF_ART\n",
            "French ---  LANGUAGE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4xiA7E2xWJt"
      },
      "source": [
        "spaCy also provides special visualization for NER through displacy. Using displacy.render() function, you can set the style=ent to visualize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "0IZc-zrQxUCJ",
        "outputId": "daf21522-12ec-4158-c253-5b07ddc1e27f"
      },
      "source": [
        "# Using displacy for visualizing NER\n",
        "from spacy import displacy\n",
        "displacy.render(doc,style='ent',jupyter=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tony Stark\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " owns the company \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    StarkEnterprises\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " . \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Emily Clark\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " works at \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Microsoft\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " and lives in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Manchester\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". She loves to read the \n",
              "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bible\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
              "</mark>\n",
              " and learn \n",
              "<mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    French\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LANGUAGE</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIoTdyxoxebf"
      },
      "source": [
        "Now that you have got a grasp on basic terms and process, let’s move on to see how named entity recognition is useful for us.\n",
        "\n",
        "Consider this article about competition in the mobile industry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ_Rkye7xYDc"
      },
      "source": [
        "mobile_industry_article=\"\"\" 30 Major mobile phone brands Compete in India – A Case Study of Success and Failures\n",
        "Is the Indian mobile market a terrible War Zone? We have more than 30 brands competing with each other. Let’s find out some insights about the world second-largest mobile bazaar.There is a massive invasion by Chinese mobile brands in India in the last four years. Some of the brands have been able to make a mark while others like Meizu, Coolpad, ZTE, and LeEco are a failure.On one side, there are brands like Sony or HTC that have quit from the Indian market on the other side we have new brands like Realme or iQOO entering the marketing in recent months.The mobile market is so competitive that some of the brands like Micromax, which had over 18% share back in 2014, now have less than 5%. Even the market leader Samsung with a 34% market share in 2014, now has a 21% share whereas Xiaomi has become a market leader. The battle is fierce and to sustain and scale-up is going to be very difficult for any new entrant.new comers in Indian Mobile MarketiQOO –They have recently (March 2020) launched the iQOO 3 in India with its first 5G phone – iQOO 3. The new brand is part of the Vivo or the BBK electronics group that also owns several other brands like Oppo, Oneplus and Realme.Realme – Realme launched the first-ever phone – Realme 1 in November 2018 and has quickly became a popular brand in India. The brand is one of the highest sellers in online space and even reached a 16% market share threatening Xiaomi’s dominance.iVoomi – In 2017, we have seen the entry of some new Chinese mobile brands likeiVoomi which focuses on the sub 10k price range, and is a popular online player. They have an association with Flipkart.Techno &amp; Infinix – Transsion Group’s Tecno and Infinix brands debuted in India in mid-2017 and are focusing on the low end and mid-range phones in the price range of Rs. 5000 to Rs. 12000.10.OR &amp; Lephone – 10.OR has a partnership with Amazon India and is an exclusive online brand with phones like 10.OR D, G and E. However, the brand is not very aggressive currently.Kult – Kult is another player who launched a very aggressively priced Kult Beyond mobile in 2017 and followed up by launching 2-3 more models.However, most of these new brands are finding it difficult to strengthen their footing in India. As big brands like Xiaomi leave no stone unturned to make things difficult.Also, it is worth noting that there is less Chinese players coming to India now. As either all the big brands have already set shop or burnt their hands and retreated to the homeland China.Chinese/ Global  Brands Which failed or are at the Verge of Failing in India?\n",
        "There are a lot more failures in the market than the success stories. Let’s first look at the failures and then we will also discuss why some brands were able to succeed in India.HTC – The biggest surprise this year for me was the failure of HTC in India. The brand has been in the country for many years, in fact, they were the first brand to launch Android mobiles. Finally HTC decided to call it a day in July 2018.LeEco – LeEco looked promising and even threatening to Xiaomi when it came to India. The company launched a series of new phones and smart TVs at affordable rates. Unfortunately, poor financial planning back home caused the brand to fail in India too.LG – The company seems to have lost focus and are doing poorly in all segments. While the budget and mid-range offering are uncompetitive, the high-end models are not preferred by buyers.Sony – Absurd pricing and lack of ability to understand the Indian buyers have caused Sony to shrink mobile operations in India. In the last 2 years, there are far fewer launches and hardly any promotions or hype around the new products.Meizu – Meizu is also a struggling brand in India and is going nowhere with the current strategy. There are hardly any popular mobiles nor a retail presence.ZTE – The company was aggressive till last year with several new phones launching under the Nubia banner, but with recent issues in the US, they have even lost the plot in India.Coolpad – I still remember the first meeting with Coolpad CEO in Mumbai when the brand started operations. There were big dreams and ambitions, but the company has not been able to deliver and keep up with the rivals in the last 1 year.Gionee – Gionee was doing well in the retail, but the infighting in the company and loss of focus from the Chinese parent company has made it a failure. The company is planning a comeback. However, we will have to wait and see when that happens.\"\"\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_prNhLMxnET"
      },
      "source": [
        "What if you want to know all the companies that are mentioned in this article?\n",
        "\n",
        "This is where Named Entity Recognition helps. You can check which tokens are organizations using label_ attribute as shown in below code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2hHXOK2xcrA",
        "outputId": "2b28541c-ef71-4a97-c98c-41b7e815cb7f"
      },
      "source": [
        "# creating spacy doc\n",
        "mobile_doc=nlp(mobile_industry_article)\n",
        "\n",
        "# List to store name of mobile companies\n",
        "list_of_org=[]\n",
        "\n",
        "# Appending entities which havel the label 'ORG' to the list\n",
        "for entity in mobile_doc.ents:\n",
        "  if entity.label_=='ORG':\n",
        "    list_of_org.append(entity.text)\n",
        "\n",
        "print(list_of_org)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Meizu', 'ZTE', 'LeEco', 'Sony', 'HTC', 'Xiaomi', 'Xiaomi', 'Flipkart', 'Techno &amp', 'Infinix – Transsion Group', 'Infinix', '12000.10.OR &amp', 'Amazon India', 'Kult', 'Kult', 'Kult Beyond', 'HTC', 'Android', 'Sony', 'Sony', 'Meizu', 'Meizu', 'ZTE', 'Nubia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFGKIEj5xsNi"
      },
      "source": [
        "You have successfully extracted list of companies that were mentioned in the article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9eWHK5Ixv2o"
      },
      "source": [
        "Let us also discuss another application. You come across many articles about theft and other crimes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76lsF9B0xpF6"
      },
      "source": [
        "# Creating a doc on news articles\n",
        "news_text=\"\"\"Indian man has allegedly duped nearly 50 businessmen in the UAE of USD 1.6 million and fled the country in the most unlikely way -- on a repatriation flight to Hyderabad, according to a media report on Saturday.Yogesh Ashok Yariava, the prime accused in the fraud, flew from Abu Dhabi to Hyderabad on a Vande Bharat repatriation flight on May 11 with around 170 evacuees, the Gulf News reported.Yariava, the 36-year-old owner of the fraudulent Royal Luck Foodstuff Trading, made bulk purchases worth 6 million dirhams (USD 1.6 million) against post-dated cheques from unsuspecting traders before fleeing to India, the daily said.\n",
        "The bought goods included facemasks, hand sanitisers, medical gloves (worth nearly 5,00,000 dirhams), rice and nuts (3,93,000 dirhams), tuna, pistachios and saffron (3,00,725 dirhams), French fries and mozzarella cheese (2,29,000 dirhams), frozen Indian beef (2,07,000 dirhams) and halwa and tahina (52,812 dirhams).\n",
        "The list of items and defrauded persons keeps getting longer as more and more victims come forward, the report said.\n",
        "The aggrieved traders have filed a case with the Bur Dubai police station.\n",
        "The traders said when the dud cheques started bouncing they rushed to the Royal Luck's office in Dubai but the shutters were down, even the fraudulent company's warehouses were empty.\"\"\"\n",
        "\n",
        "news_doc=nlp(news_text)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2-sAo36x2Sg"
      },
      "source": [
        "While using this for a case study, you might need to to avoid use of original names, companies and places. How can you do it ?\n",
        "\n",
        "Write a function which will scan the text for named entities which have the labels PERSON , ORG and GPE. These tokens can be replaced by “UNKNOWN”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "ADl1eeurxySt",
        "outputId": "171ffbf1-e260-49a5-8251-0b537e15c850"
      },
      "source": [
        "# Function to identify  if tokens are named entities and replace them with UNKNOWN\n",
        "def remove_details(word):\n",
        "  if word.ent_type_ =='PERSON' or word.ent_type_=='ORG' or word.ent_type_=='GPE':\n",
        "    return ' UNKNOWN '\n",
        "  return word.string\n",
        "\n",
        "\n",
        "# Function where each token of spacy doc is passed through remove_deatils()\n",
        "def update_article(doc):\n",
        "  # iterrating through all entities\n",
        "  for ent in doc.ents:\n",
        "    ent.merge()\n",
        "  # Passing each token through remove_details() function.\n",
        "  tokens = map(remove_details,doc)\n",
        "  return ''.join(tokens)\n",
        "\n",
        "# Passing our news_doc to the function update_article()\n",
        "update_article(news_doc)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Indian man has allegedly duped nearly 50 businessmen in the  UNKNOWN of USD 1.6 million and fled the country in the most unlikely way -- on a repatriation flight to  UNKNOWN , according to a media report on Saturday. UNKNOWN , the prime accused in the fraud, flew from  UNKNOWN to  UNKNOWN on a Vande Bharat repatriation flight on May 11 with around 170 evacuees,  UNKNOWN reported. UNKNOWN , the 36-year-old owner of the fraudulent  UNKNOWN , made bulk purchases worth 6 million dirhams (USD 1.6 million) against post-dated cheques from unsuspecting traders before fleeing to  UNKNOWN , the daily said.\\nThe bought goods included facemasks, hand sanitisers, medical gloves (worth nearly 5,00,000 dirhams), rice and nuts (3,93,000 dirhams), tuna, pistachios and saffron (3,00,725 dirhams), French fries and mozzarella cheese (2,29,000 dirhams), frozen Indian beef (2,07,000 dirhams) and halwa and  UNKNOWN (52,812 dirhams).\\nThe list of items and defrauded persons keeps getting longer as more and more victims come forward, the report said.\\nThe aggrieved traders have filed a case with the Bur Dubai police station.\\nThe traders said when the  UNKNOWN cheques started bouncing they rushed to  UNKNOWN office in  UNKNOWN but the shutters were down, even the fraudulent company's warehouses were empty.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWOLtmjzyDZ7"
      },
      "source": [
        "Using Matcher of spacy you can identify token patterns as seen above. But when you have a phrase to be matched, using Matcher will take a lot of time and is not efficient.\n",
        "\n",
        "spaCy provides PhraseMatcher which can be used when you have a large number of terms(single or multi-tokens) to be matched in a text document. Writing patterns for Matcher is very difficult in this case. PhraseMatcher solves this problem, as you can pass Doc patterns rather than Token patterns.\n",
        "\n",
        "The procedure to use PhraseMatcher is very similar to Matcher.\n",
        "\n",
        "Initialize a PhraseMatcher object with a vocab.\n",
        "Define the terms you want to match\n",
        "Add the pattern to the matcher\n",
        "Run the text through the matcher to extract the matching positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngqm6mVOx45t"
      },
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "# PhraseMatcher\n",
        "# After importing , first you need to initialize the PhraseMatcher with vocab through below command\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si9-UfpfyJYE"
      },
      "source": [
        "# Terms to match\n",
        "terms_list = ['Bruce Wayne', 'Tony Stark', 'Batman', 'Harry Potter', 'Severus Snape']"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkNE7rm7yMld"
      },
      "source": [
        "# Make a list of docs\n",
        "patterns = [nlp.make_doc(text) for text in terms_list]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWSqNLbMySNY"
      },
      "source": [
        "You can add the pattern to your matcher through matcher.add() method.\n",
        "\n",
        "The inputs for the function are – A custom ID for your matcher, optional parameter for callable function, pattern list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64zeQhD9yO6o"
      },
      "source": [
        "matcher.add(\"phrase_matcher\", None, *patterns)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEt5PA27yb4E"
      },
      "source": [
        "Now you can apply your matcher to your spacy text document. Below, you have a text article on prominent fictional characters and their creators.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhoFASX_yXQM"
      },
      "source": [
        "# Matcher Object\n",
        "fictional_char_doc = nlp(\"\"\"Superman (first appearance: 1938)  Created by Jerry Siegal and Joe Shuster for Action Comics #1 (DC Comics).Mickey Mouse (1928)  Created by Walt Disney and Ub Iworks for Steamboat Willie.Bugs Bunny (1940)  Created by Warner Bros and originally voiced by Mel Blanc.Batman (1939) Created by Bill Finger and Bob Kane for Detective Comics #27 (DC Comics).\n",
        "Dorothy Gale (1900)  Created by L. Frank Baum for novel The Wonderful Wizard of Oz. Later portrayed by Judy Garland in the 1939 film adaptation.Darth Vader (1977) Created by George Lucas for Star Wars IV: A New Hope.The Tramp (1914)  Created and portrayed by Charlie Chaplin for Kid Auto Races at Venice.Peter Pan (1902)  Created by J.M. Barrie for novel The Little White Bird.\n",
        "Indiana Jones (1981)  Created by George Lucas for Raiders of the Lost Ark. Portrayed by Harrison Ford.Rocky Balboa (1976)  Created and portrayed by Sylvester Stallone for Rocky.Vito Corleone (1969) Created by Mario Puzo for novel The Godfather. Later portrayed by Marlon Brando and Robert DeNiro in Coppola’s film adaptation.Han Solo (1977) Created by George Lucas for Star Wars IV: A New Hope. \n",
        "Portrayed most famously by Harrison Ford.Homer Simpson (1987)  Created by Matt Groening for The Tracey Ullman Show, later The Simpsons as voiced by Dan Castellaneta.Archie Bunker (1971) Created by Norman Lear for All in the Family. Portrayed by Carroll O’Connor.Norman Bates (1959) Created by Robert Bloch for novel Psycho.  Later portrayed by Anthony Perkins in Hitchcock’s film adaptation.King Kong (1933) \n",
        "Created by Edgar Wallace and Merian C Cooper for the film King Kong.Lucy Ricardo (1951) Portrayed by Lucille Ball for I Love Lucy.Spiderman (1962)  Created by Stan Lee and Steve Ditko for Amazing Fantasy #15 (Marvel Comics).Barbie (1959)  Created by Ruth Handler for the toy company Mattel Spock (1964)  Created by Gene Roddenberry for Star Trek. Portrayed most famously by Leonard Nimoy.\n",
        "Godzilla (1954) Created by Tomoyuki Tanaka, Ishiro Honda, and Eiji Tsubaraya for the film Godzilla.The Joker (1940)  Created by Jerry Robinson, Bill Finger, and Bob Kane for Batman #1 (DC Comics)Winnie-the-Pooh (1924)  Created by A.A. Milne for verse book When We Were Young.Popeye (1929)  Created by E.C. Segar for comic strip Thimble Theater (King Features).Tarzan (1912) Created by Edgar Rice Burroughs for the novel Tarzan of the Apes.Forrest Gump (1986)  Created by Winston Groom for novel Forrest Gump.  Later portrayed by Tom Hanks in Zemeckis’ film adaptation.Hannibal Lector (1981)  Created by Thomas Harris for the novel Red Dragon. Portrayed most famously by Anthony Hopkins in the 1991 Jonathan Demme film The Silence of the Lambs.\n",
        "Big Bird (1969) Created by Jim Henson and portrayed by Carroll Spinney for Sesame Street.Holden Caulfield (1945) Created by J.D. Salinger for the Collier’s story “I’m Crazy.”  Reworked into the novel The Catcher in the Rye in 1951.Tony Montana (1983)  Created by Oliver Stone for film Scarface.  Portrayed by Al Pacino.Tony Soprano (1999)  Created by David Chase for The Sopranos. Portrayed by James Gandolfini.\n",
        "The Terminator (1984)  Created by James Cameron and Gale Anne Hurd for The Terminator. Portrayed by Arnold Schwarzenegger.Jon Snow (1996)  Created by George RR Martin for the novel The Game of Thrones.  Portrayed by Kit Harrington.Charles Foster Kane (1941)  Created and portrayed by Orson Welles for Citizen Kane.Scarlett O’Hara (1936)  Created by Margaret Mitchell for the novel Gone With the Wind. Portrayed most famously by Vivien Leigh \n",
        "for the 1939 Victor Fleming film adaptation.Marty McFly (1985) Created by Robert Zemeckis and Bob Gale for Back to the Future. Portrayed by Michael J. Fox.Rick Blaine (1940)  Created by Murray Burnett and Joan Alison for the unproduced stage play Everybody Comes to Rick’s. Later portrayed by Humphrey Bogart in Michael Curtiz’s film adaptation Casablanca.Man With No Name (1964)  Created by Sergio Leone for A Fistful of Dollars, which was adapted from a ronin character in Kurosawa’s Yojimbo (1961).  Portrayed by Clint Eastwood.Charlie Brown (1948)  Created by Charles M. Shultz for the comic strip L’il Folks; popularized two years later in Peanuts.E.T. (1982)  Created by Melissa Mathison for the film E.T.: the Extra-Terrestrial.Arthur Fonzarelli (1974)  Created by Bob Brunner for the show Happy Days. Portrayed by Henry Winkler.)Phillip Marlowe (1939)  Created by Raymond Chandler for the novel The Big Sleep.Jay Gatsby (1925)  Created by F. Scott Fitzgerald for the novel The Great Gatsby.Lassie (1938) Created by Eric Knight for a Saturday Evening Post story, later turned into the novel Lassie Come-Home in 1940, film adaptation in 1943, and long-running television show in 1954.  Most famously portrayed by the dog Pal.\n",
        "Fred Flintstone (1959)  Created by William Hanna and Joseph Barbera for The Flintstones. Voiced most notably by Alan Reed. Rooster Cogburn (1968)  Created by Charles Portis for the novel True Grit. Most famously portrayed by John Wayne in the 1969 film adaptation. Atticus Finch (1960)  Created by Harper Lee for the novel To Kill a Mockingbird.  (Appeared in the earlier work Go Set A Watchman, though this was not published until 2015)  Portrayed most famously by Gregory Peck in the Robert Mulligan film adaptation. Kermit the Frog (1955)  Created and performed by Jim Henson for the show Sam and Friends. Later popularized in Sesame Street (1969) and The Muppet Show (1976) George Bailey (1943)  Created by Phillip Van Doren Stern (then as George Pratt) for the short story The Greatest Gift. Later adapted into Capra’s It’s A Wonderful Life, starring James Stewart as the renamed George Bailey. Yoda (1980) Created by George Lucas for The Empire Strikes Back. Sam Malone (1982)  Created by Glen and Les Charles for the show Cheers.  Portrayed by Ted Danson. Zorro (1919)  Created by Johnston McCulley for the All-Story Weekly pulp magazine story The Curse of Capistrano.Later adapted to the Douglas Fairbanks’ film The Mark of Zorro (1920).Moe, Larry, and Curly (1928)  Created by Ted Healy for the vaudeville act Ted Healy and his Stooges. Mary Poppins (1934)  Created by P.L. Travers for the children’s book Mary Poppins. Ron Burgundy (2004)  Created by Will Ferrell and Adam McKay for the film Anchorman: The Legend of Ron Burgundy.  Portrayed by Will Ferrell. Mario (1981)  Created by Shigeru Miyamoto for the video game Donkey Kong. Harry Potter (1997)  Created by J.K. Rowling for the novel Harry Potter and the Philosopher’s Stone. The Dude (1998)  Created by Ethan and Joel Coen for the film The Big Lebowski. Portrayed by Jeff Bridges.\n",
        "Gandalf (1937)  Created by J.R.R. Tolkien for the novel The Hobbit. The Grinch (1957)  Created by Dr. Seuss for the story How the Grinch Stole Christmas! Willy Wonka (1964)  Created by Roald Dahl for the children’s novel Charlie and the Chocolate Factory. The Hulk (1962)  Created by Stan Lee and Jack Kirby for The Incredible Hulk #1 (Marvel Comics) Scooby-Doo (1969)  Created by Joe Ruby and Ken Spears for the show Scooby-Doo, Where Are You! George Costanza (1989)  Created by Larry David and Jerry Seinfeld for the show Seinfeld.  Portrayed by Jason Alexander.Jules Winfield (1994)  Created by Quentin Tarantino for the film Pulp Fiction. Portrayed by Samuel L. Jackson. John McClane (1988)  Based on the character Detective Joe Leland, who was created by Roderick Thorp for the novel Nothing Lasts Forever. Later adapted into the John McTernan film Die Hard, starring Bruce Willis as McClane. Ellen Ripley (1979)  Created by Don O’cannon and Ronald Shusett for the film Alien.  Portrayed by Sigourney Weaver. Ralph Kramden (1951)  Created and portrayed by Jackie Gleason for “The Honeymooners,” which became its own show in 1955.Edward Scissorhands (1990)  Created by Tim Burton for the film Edward Scissorhands.  Portrayed by Johnny Depp.Eric Cartman (1992)  Created by Trey Parker and Matt Stone for the animated short Jesus vs Frosty.  Later developed into the show South Park, which premiered in 1997.  Voiced by Trey Parker.\n",
        "Walter White (2008)  Created by Vince Gilligan for Breaking Bad.  Portrayed by Bryan Cranston. Cosmo Kramer (1989)  Created by Larry David and Jerry Seinfeld for Seinfeld.  Portrayed by Michael Richards.Pikachu (1996)  Created by Atsuko Nishida and Ken Sugimori for the Pokemon video game and anime franchise.Michael Scott (2005)  Based on a character from the British series The Office, created by Ricky Gervais and Steven Merchant.  Portrayed by Steve Carell.Freddy Krueger (1984)  Created by Wes Craven for the film A Nightmare on Elm Street. Most famously portrayed by Robert Englund.\n",
        "Captain America (1941)  Created by Joe Simon and Jack Kirby for Captain America Comics #1 (Marvel Comics)Goku (1984)  Created by Akira Toriyama for the manga series Dragon Ball Z.Bambi (1923)  Created by Felix Salten for the children’s book Bambi, a Life in the Woods. Later adapted into the Disney film Bambi in 1942.Ronald McDonald (1963) Created by Williard Scott for a series of television spots.Waldo/Wally (1987) Created by Martin Hanford for the children’s book Where’s Wally? (Waldo in US edition) Frasier Crane (1984)  Created by Glen and Les Charles for Cheers.  Portrayed by Kelsey Grammar.Omar Little (2002)  Created by David Simon for The Wire.Portrayed by Michael K. Williams.\n",
        "Wolverine (1974)  Created by Roy Thomas, Len Wein, and John Romita Sr for The Incredible Hulk #180 (Marvel Comics) Jason Voorhees (1980)  Created by Victor Miller for the film Friday the 13th. Betty Boop (1930)  Created by Max Fleischer and the Grim Network for the cartoon Dizzy Dishes. Bilbo Baggins (1937)  Created by J.R.R. Tolkien for the novel The Hobbit. Tom Joad (1939)  Created by John Steinbeck for the novel The Grapes of Wrath. Later adapted into the 1940 John Ford film and portrayed by Henry Fonda.Tony Stark (Iron Man) (1963)  Created by Stan Lee, Larry Lieber, Don Heck and Jack Kirby for Tales of Suspense #39 (Marvel Comics)Porky Pig (1935)  Created by Friz Freleng for the animated short film I Haven’t Got a Hat. Voiced most famously by Mel Blanc.Travis Bickle (1976)  Created by Paul Schrader for the film Taxi Driver. Portrayed by Robert De Niro.\n",
        "Hawkeye Pierce (1968)  Created by Richard Hooker for the novel MASH: A Novel About Three Army Doctors.  Famously portrayed by both Alan Alda and Donald Sutherland. Don Draper (2007)  Created by Matthew Weiner for the show Mad Men.  Portrayed by Jon Hamm. Cliff Huxtable (1984)  Created and portrayed by Bill Cosby for The Cosby Show. Jack Torrance (1977)  Created by Stephen King for the novel The Shining. Later adapted into the 1980 Stanley Kubrick film and portrayed by Jack Nicholson. Holly Golightly (1958)  Created by Truman Capote for the novella Breakfast at Tiffany’s.  Later adapted into the 1961 Blake Edwards films starring Audrey Hepburn as Holly. Shrek (1990)  Created by William Steig for the children’s book Shrek! Later adapted into the 2001 film starring Mike Myers as the titular character. Optimus Prime (1984)  Created by Dennis O’Neil for the Transformers toy line.Sonic the Hedgehog (1991)  Created by Naoto Ohshima and Yuji Uekawa for the Sega Genesis game of the same name.Harry Callahan (1971)  Created by Harry Julian Fink and R.M. Fink for the movie Dirty Harry.  Portrayed by Clint Eastwood.Bubble: Hercule Poirot, Tyrion Lannister, Ron Swanson, Cercei Lannister, J.R. Ewing, Tyler Durden, Spongebob Squarepants, The Genie from Aladdin, Pac-Man, Axel Foley, Terry Malloy, Patrick Bateman\n",
        "Pre-20th Century: Santa Claus, Dracula, Robin Hood, Cinderella, Huckleberry Finn, Odysseus, Sherlock Holmes, Romeo and Juliet, Frankenstein, Prince Hamlet, Uncle Sam, Paul Bunyan, Tom Sawyer, Pinocchio, Oliver Twist, Snow White, Don Quixote, Rip Van Winkle, Ebenezer Scrooge, Anna Karenina, Ichabod Crane, John Henry, The Tooth Fairy,\n",
        "Br’er Rabbit, Long John Silver, The Mad Hatter, Quasimodo \"\"\")\n",
        "\n",
        "\n",
        "character_matches = matcher(fictional_char_doc)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5UUfOPZyjNS"
      },
      "source": [
        "The PhraseMatcher returns a list of (match_id, start, end) tuples, describing the matches. A match tuple describes a span doc[start:end].\n",
        "\n",
        "The match_id refers to the string ID of the match pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfbA-WpfygIP",
        "outputId": "988cd4a1-36f1-4cb1-c750-a811768615e8"
      },
      "source": [
        "# Matching positions\n",
        "character_matches"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(520014689628841516, 56, 57),\n",
              " (520014689628841516, 449, 450),\n",
              " (520014689628841516, 1352, 1354),\n",
              " (520014689628841516, 1365, 1367),\n",
              " (520014689628841516, 2084, 2086)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km0Mve25yqTm"
      },
      "source": [
        "You can see that 3 of the terms have been found in the text, but we dont know what they are. For that , you need to extract the Span using start and end as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2hlxINIylD0",
        "outputId": "c6bdebe1-8457-4a1b-9b9f-d5064927294b"
      },
      "source": [
        "# Matched items\n",
        "for match_id, start, end in character_matches:\n",
        "    span = fictional_char_doc[start:end]\n",
        "    print(span.text)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batman\n",
            "Batman\n",
            "Harry Potter\n",
            "Harry Potter\n",
            "Tony Stark\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccyx03VAywnB"
      },
      "source": [
        "You can see that ‘Harry Potter’ and ‘Batman’ were mentioned twice ,\n",
        "‘Tony Stark’ once, but the other terms didn’t match.\n",
        "\n",
        "Another useful feature of PhraseMatcher is that while intializing the matcher, you have an option to use the parameter attr, using which you can set rules for how the matching has to happen.\n",
        "\n",
        "How to use attr?\n",
        "\n",
        "Setting a attr to match on will change the token attributes that will be compared to determine a match. For example, if you use attr='LOWER', then case-insensitive matching will happen.\n",
        "\n",
        "For understanding, I shall demonstrate it in the below example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wepZi8UdytH_",
        "outputId": "64ea574f-40d3-4205-b94d-5402f94c71ff"
      },
      "source": [
        "# Using the attr parameter as 'LOWER'\n",
        "case_insensitive_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "\n",
        "# Creating doc &amp; pattern\n",
        "my_doc=nlp('I wish to visit new york city')\n",
        "terms=['New York']\n",
        "pattern=[nlp(term) for term in terms]\n",
        "\n",
        "# adding pattern to the matcher\n",
        "case_insensitive_matcher.add(\"matcher\",None,*pattern)\n",
        "\n",
        "# applying matcher to the doc\n",
        "my_matches=case_insensitive_matcher(my_doc)\n",
        "\n",
        "for match_id,start,end in my_matches:\n",
        "  span=my_doc[start:end]\n",
        "  print(span.text)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new york\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD_NmHVry1Kc"
      },
      "source": [
        "You can observe that irrespective the difference in the case, the phrase was successfully matched.\n",
        "\n",
        "Let’s see a more useful case.\n",
        "\n",
        "If you set the attr='SHAPE', then matching will be based on the shape of the terms in pattern .\n",
        "\n",
        "This can be used to match URLs, dates of specific format, time-formats, where the shape will be same. Let us consider a text having information about various radio channels.\n",
        "\n",
        "You want to extract the channels (in the form of ddd.d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4bsx0wlyyK4"
      },
      "source": [
        "my_doc = nlp('From 8 am , Mr.X will be speaking on your favorite chanel 191.1. Afterward there shall be an exclusive interview with actor Vijay on channel 194.1 . Hope you are having a great day. Call us on 666666')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKp4uDzxzN7_"
      },
      "source": [
        "# Let us create the pattern. You need to pass an example radio channel of the desired shape as pattern to the matcher.\n",
        "pattern=nlp('154.6')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnkHuseLzU6j"
      },
      "source": [
        "Your pattern is ready , now initialize the PhraseMatcher with attribute set as \"SHAPE\".. Then add the pattern to matcher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3OxWTRVzSuS"
      },
      "source": [
        "# Initializing the matcher and adding pattern\n",
        "from spacy.matcher import PhraseMatcher\n",
        "pincode_matcher= PhraseMatcher(nlp.vocab,attr=\"SHAPE\")\n",
        "pincode_matcher.add(\"pincode_matching\", None, pattern)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaE9NRuazYYn"
      },
      "source": [
        "You can apply the matcher to your doc as usual and print the matching phrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF8NTT-RzWoi",
        "outputId": "3a6f9425-bc52-4023-e01c-bf293ebb4ff3"
      },
      "source": [
        "# Applying matcher on doc\n",
        "matches = pincode_matcher(my_doc)\n",
        "\n",
        "# Printing the matched phrases\n",
        "for match_id, start, end in matches:\n",
        "  span = my_doc[start:end]\n",
        "  print(span.text)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "191.1\n",
            "194.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZrlBODnzeGl"
      },
      "source": [
        "Entity Ruler is intetesting and very useful.\n",
        "\n",
        "While trying to detect entities, some times certain names or organizations are not recognized by default. It might be because they are small scale or rare. Wouldn’t it be better to improve accuracy of our doc.ents_ method ?\n",
        "\n",
        "spaCy provides a more advanced component EntityRuler that let’s you match named entities based on pattern dictionaries. Overall, it makes Named Entity Recognition more efficient.\n",
        "\n",
        "It is a pipeline supported component and can be imported as shown below ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHug5logzaBh"
      },
      "source": [
        "from spacy.pipeline import EntityRuler\n",
        "# Initialize\n",
        "ruler = EntityRuler(nlp)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpGWeQZczmtM"
      },
      "source": [
        "What type of patterns do you pass to the EntityRuler ?\n",
        "\n",
        "Basically, you need to pass a list of dictionaries, where each dictionary represents a pattern to be matched.\n",
        "\n",
        "Each dictionary has two keys \"label\" and \"pattern\".\n",
        "\n",
        "label : Holds the entity type as values eg: PERSON, GPE, etc\n",
        "pattern: Holds the the matcher pattern as values eg: John, Calcutta, etc\n",
        "For example, let us consider a situation where you want to add certain book names under the entity label WORK_OF_ART.\n",
        "\n",
        "What will be your pattern ?\n",
        "\n",
        "My label will be WORK_OF_ART and pattern will contain the book names I wish to add. Below code demonstrates the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeGJeuaVzjz8"
      },
      "source": [
        "pattern=[{\"label\": \"WORK_OF_ART\", \"pattern\": \"My guide to statistics\"}]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Z18RBJzqYQ"
      },
      "source": [
        "You can add pattern to the ruler through add_patterns() function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Krmy_ezo1b"
      },
      "source": [
        "ruler.add_patterns(pattern)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw04WKz9z6C1"
      },
      "source": [
        "How can you apply the EntityRuler to your text ?\n",
        "\n",
        "You can add it to the nlp model through add_pipe() function. It Adds the ruler component to the processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyDSJG4Hzr4k"
      },
      "source": [
        "# Add entity ruler to the NLP pipeline. \n",
        "# NLP pipeline is a sequence of NLP tasks that spaCy performs for a given text\n",
        "# More on pipelines coming in future section in this post.\n",
        "nlp.add_pipe(ruler)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRAgjW-Fz-YA"
      },
      "source": [
        "Now , the EntityRuler is incorporated into nlp. You can pass the text document to nlp to create a spacy doc . As the ruler is already added, by default “My guide to statistics” will be recognized as named entities under category WORK_OF_ART.\n",
        "\n",
        "You can verify it through below code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3eMUPmRz8Uk",
        "outputId": "57024274-6902-4ad1-993a-18493ba732a7"
      },
      "source": [
        "# Extract the custom entity type \n",
        "doc = nlp(\" I recently published my work fanfiction by Dr.X . Right now I'm studying the book of my friend .You should try My guide to statistics for clear concepts.\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('My guide to statistics', 'WORK_OF_ART')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlE3R93N0CKP"
      },
      "source": [
        "You have successfuly enhanced the named entity recoginition. It is possible to train spaCy to detect new entities it has not seen as well.\n",
        "\n",
        "EntityRuler has many amazing features, you’ll run into them later in this article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyRztq4p0FBP"
      },
      "source": [
        "Word Vectors are numerical vector representations of words and documents. The numeric form helps understand the semantics about the word and can be used for NLP tasks such as classification.\n",
        "\n",
        "Because, vector representation of words that are similar in meaning and context appear closer together.\n",
        "\n",
        "spaCy models support inbuilt vectors that can be accessed through directly through the attributes of Token and Doc. How can you check if the model supports tokens with vectors ?\n",
        "\n",
        "First, load a spaCy model of your choice. Here, I am using the medium model for english en_core_web_md. Next, tokenize your text document with nlp boject of spacy model.\n",
        "\n",
        "You can check if a token has in-buit vector through Token.has_vector attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE0sJVft0AFc",
        "outputId": "b38292e4-7290-47c7-b84a-da32ea2bec42"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp37-none-any.whl size=98051305 sha256=9c62d4770cb0a9bea1f42ab94dbf15508593afeee21d748443d9911864c0c51e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7xb4s1c5/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsDcHfhS0Umh"
      },
      "source": [
        "**When running this in Colab, if the cell block below throws an error, go to Runtime > Restart runtime. Then run the below cell block again and it will work.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j9bh-DM0IuL",
        "outputId": "b1b68b6f-8631-4de7-848d-543c1b4b2f48"
      },
      "source": [
        "# Check if word vector is available\n",
        "import spacy\n",
        "\n",
        "# Loading a spacy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "tokens = nlp(\"I am an excellent cook\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(token.text ,' ',token.has_vector)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I   True\n",
            "am   True\n",
            "an   True\n",
            "excellent   True\n",
            "cook   True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RetXWrkD0OLb"
      },
      "source": [
        "You can see that all tokens in above text have a vector. It is because these words are pre-existing or the model has been trained on them. Let’s see what is the result when the text has some non-existent / made up word ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLeNxdnu0KeC",
        "outputId": "3f6b8006-93a1-480c-f908-637e34de7044"
      },
      "source": [
        "# Check if word vector is available\n",
        "tokens=nlp(\"I wish to go to hogwarts lolXD \")\n",
        "for token in tokens:\n",
        "  print(token.text,' ',token.has_vector)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I   True\n",
            "wish   True\n",
            "to   True\n",
            "go   True\n",
            "to   True\n",
            "hogwarts   True\n",
            "lolXD   False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIKKHzZ20mDv"
      },
      "source": [
        "The word “lolXD” is not a part of the model’s vocabulary, hence it does not have a vector. (Remember this issue from tfidf when our corpus we used to train the tfidf vectorizer did not have many words?)\n",
        "\n",
        "How to access the vector of the tokens?\n",
        "\n",
        "You can access through token.vector method. Also ,token.vector_norm attribute stores L2 norm of the token’s vector representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bgdgUoE0hEO",
        "outputId": "b65199d8-17c3-4ffe-f83b-73fe9fd69bfc"
      },
      "source": [
        "# Extract the word Vector\n",
        "tokens=nlp(\"I wish to go to hogwarts lolXD \")\n",
        "for token in tokens:\n",
        "  print(token.text,' ',token.vector_norm)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I   6.4231944\n",
            "wish   5.1652417\n",
            "to   4.74484\n",
            "go   5.05723\n",
            "to   4.74484\n",
            "hogwarts   7.4110312\n",
            "lolXD   0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz1Czr2-0ww6",
        "outputId": "00af1c2d-31ae-4b83-eb34-09becc0b4c81"
      },
      "source": [
        "# actual vector\n",
        "for token in tokens:\n",
        "  print(token.vector)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.8733e-01  4.0595e-01 -5.1174e-01 -5.5482e-01  3.9716e-02  1.2887e-01\n",
            "  4.5137e-01 -5.9149e-01  1.5591e-01  1.5137e+00 -8.7020e-01  5.0672e-02\n",
            "  1.5211e-01 -1.9183e-01  1.1181e-01  1.2131e-01 -2.7212e-01  1.6203e+00\n",
            " -2.4884e-01  1.4060e-01  3.3099e-01 -1.8061e-02  1.5244e-01 -2.6943e-01\n",
            " -2.7833e-01 -5.2123e-02 -4.8149e-01 -5.1839e-01  8.6262e-02  3.0818e-02\n",
            " -2.1253e-01 -1.1378e-01 -2.2384e-01  1.8262e-01 -3.4541e-01  8.2611e-02\n",
            "  1.0024e-01 -7.9550e-02 -8.1721e-01  6.5621e-03  8.0134e-02 -3.9976e-01\n",
            " -6.3131e-02  3.2260e-01 -3.1625e-02  4.3056e-01 -2.7270e-01 -7.6020e-02\n",
            "  1.0293e-01 -8.8653e-02 -2.9087e-01 -4.7214e-02  4.6036e-02 -1.7788e-02\n",
            "  6.4990e-02  8.8451e-02 -3.1574e-01 -5.8522e-01  2.2295e-01 -5.2785e-02\n",
            " -5.5981e-01 -3.9580e-01 -7.9849e-02 -1.0933e-02 -4.1722e-02 -5.5576e-01\n",
            "  8.8707e-02  1.3710e-01 -2.9873e-03 -2.6256e-02  7.7330e-02  3.9199e-01\n",
            "  3.4507e-01 -8.0130e-02  3.3451e-01  2.7063e-01 -2.4544e-02  7.2576e-02\n",
            " -1.8120e-01  2.3693e-01  3.9977e-01  4.5012e-01  2.7179e-02  2.7400e-01\n",
            "  1.4791e-01 -5.8324e-03  9.5910e-01 -1.0129e+00  2.0699e-01  1.8237e-01\n",
            " -2.5234e-01 -2.6261e-01 -3.4799e-01 -2.4051e-02  4.4470e-01  5.9226e-02\n",
            "  4.5561e-01  1.9700e-01 -4.8327e-01  8.9523e-02 -2.2373e-01 -1.5654e-01\n",
            "  2.1578e-01  1.1673e-01  8.2006e-02 -8.0735e-01  2.3903e-01 -5.1304e-01\n",
            " -3.3888e-01 -3.1499e-01 -1.7272e-01 -6.7020e-01  2.7096e-01 -4.3241e-01\n",
            "  4.3103e-02  2.1233e-02  1.3350e-02 -6.3938e-02 -2.4957e-01 -2.4938e-01\n",
            "  3.4812e-01 -7.1321e-02  2.3375e-01 -9.5384e-02  5.2488e-01  6.8175e-01\n",
            " -1.0214e-01 -1.4914e-01 -7.5697e-02  1.7248e-01  2.5440e-01  1.5760e-01\n",
            " -5.9125e-01  2.4300e-01  6.3962e-01 -9.3280e-02 -2.7914e-01 -6.6262e-02\n",
            " -6.7170e-02 -4.0929e-01 -3.0300e+00  1.8250e-01  2.0113e-01  6.0628e-02\n",
            " -2.4769e-01  5.5324e-02 -4.9106e-01  3.1544e-01 -3.4231e-01 -6.3766e-01\n",
            " -3.6129e-01 -5.9029e-02  1.5510e-01  4.4577e-02  2.3572e-01 -1.7095e-01\n",
            " -2.2749e-01 -2.3184e-02  2.3868e-01  2.8170e-02  4.2965e-01 -1.2458e-01\n",
            " -3.6972e-02  2.0061e-01 -3.1405e-01 -8.5287e-02 -3.3496e-01 -9.7047e-02\n",
            " -1.4388e-01  1.1147e-01 -4.5232e-01 -2.4217e-01 -1.8245e-01 -6.7292e-01\n",
            "  2.1933e-02 -5.4816e-02 -4.6508e-01  4.7767e-01 -2.4752e-01 -1.5790e-01\n",
            "  1.1817e-01  5.6851e-02 -4.9151e-01  1.5496e-01  1.6425e-02  4.1650e-02\n",
            " -3.4990e-01 -1.5979e-01  3.9705e-01  2.2963e-01  2.4688e-01  1.9567e-02\n",
            " -2.8802e-01 -6.9983e-01  3.2744e-01  1.0833e-01  2.4945e-01 -7.8653e-01\n",
            " -6.1379e-02 -3.7359e-01 -1.1603e-01 -2.4950e-01  1.0161e-01  3.3994e-02\n",
            "  1.5650e-01  2.1344e-01 -1.1094e-01 -5.7687e-03  1.7869e-01 -1.0127e-01\n",
            " -1.6891e-02  3.0001e-01 -3.4116e-01 -3.2390e-02  4.2514e-02  1.1850e-01\n",
            " -1.8337e-01 -6.2865e-01 -2.8021e-01  4.2351e-01  1.1277e-01  1.2121e-03\n",
            "  1.5710e-01 -3.6321e-01 -4.9251e-01  1.1653e-01  2.4024e-01  1.7712e-01\n",
            "  6.8700e-02 -4.4137e-01 -2.9877e-01 -1.2071e-02  2.8325e-01  1.0668e-01\n",
            " -1.8859e-01 -4.1345e-01 -3.4090e-01  4.7236e-02 -3.8309e-01  4.3572e-01\n",
            "  2.4505e-01  2.7337e-01 -7.3038e-02  4.2514e-01 -3.2455e-02 -3.5211e-01\n",
            "  4.5691e-01  1.9433e-01 -1.5230e-01  4.2675e-01  2.8795e-01 -5.5969e-01\n",
            " -1.3031e-01  8.9844e-02  4.2605e-01 -1.9632e-01 -7.1989e-02 -8.0189e-02\n",
            " -3.0425e-01 -4.6190e-01  2.8178e-01 -9.9872e-02  3.5097e-01  1.6123e-01\n",
            " -3.6548e-02 -3.6739e-01 -1.9819e-02  3.2130e-01  1.7479e-01  2.5175e-01\n",
            " -7.6439e-03 -9.3786e-02 -3.7852e-01  4.3725e-01  2.1288e-01  2.5096e-01\n",
            " -1.9613e-01 -2.8865e-01 -5.6726e-03  4.2795e-01  2.0625e-01 -3.7701e-02\n",
            " -1.2200e-01 -7.9253e-02 -1.0290e-01  1.0558e-02  4.9880e-01  2.5382e-01\n",
            "  1.5526e-01  1.7951e-03  1.1633e-01  7.9300e-02 -3.9142e-01 -3.2483e-01\n",
            "  6.3451e-01 -1.8910e-01  5.4050e-02  1.6495e-01  1.8757e-01  5.3874e-01]\n",
            "[ 2.7620e-02 -5.7683e-02 -4.1660e-01 -1.0294e-01 -1.4487e-01 -4.6075e-02\n",
            "  7.6186e-02 -5.0749e-01  1.0197e-01  2.1410e+00 -4.6990e-01  8.8692e-02\n",
            "  1.4324e-01 -2.2832e-02 -1.4183e-01 -3.1921e-02 -1.9025e-01  8.2235e-01\n",
            " -2.0831e-01  3.6195e-01 -2.2419e-02  1.9220e-02 -2.1075e-01  1.4462e-02\n",
            "  1.1503e-01  3.9502e-01 -1.9324e-01 -2.1402e-01  1.3174e-01 -2.7991e-02\n",
            " -1.4727e-01  1.8810e-01 -2.1657e-01  4.2056e-01 -3.4731e-01 -2.6248e-01\n",
            "  2.5284e-01  3.5359e-02 -6.7844e-02  2.1340e-02 -3.1929e-01 -8.5099e-02\n",
            "  4.5493e-02  5.7101e-02 -2.6107e-02  1.3337e-01 -1.9127e-01  8.1717e-02\n",
            "  2.3087e-01  8.6612e-02 -2.6333e-01 -2.6220e-01  1.2490e-01 -9.5334e-03\n",
            "  5.5589e-01  3.6577e-02 -2.5281e-01 -1.6389e-01 -1.4422e-01  9.9330e-02\n",
            " -3.1903e-01 -2.6667e-01 -5.2721e-01 -8.9154e-02  1.6958e-01  2.9245e-01\n",
            " -2.0795e-01  1.3516e-01  2.3276e-01 -1.0826e-01  1.8010e-01  9.3658e-02\n",
            "  1.6712e-01 -1.5873e-01  5.3394e-01  3.0855e-01 -5.0001e-02 -5.1562e-02\n",
            " -2.5824e-01  1.1046e-01 -2.6026e-01  3.6692e-01 -4.3087e-01  6.4423e-02\n",
            "  2.4849e-01 -4.7797e-01 -7.6765e-02 -2.6309e-01  6.9257e-01 -2.3715e-01\n",
            " -3.9937e-01  1.0118e-01 -7.2125e-03  3.9733e-01  9.2629e-02 -7.9730e-02\n",
            " -8.6463e-02 -1.1807e-01 -3.0278e-01  1.8806e-01  2.8015e-01 -5.0167e-01\n",
            "  4.4538e-01  2.0943e-01  2.3203e-01 -5.7380e-01  2.3517e-02 -2.9192e-01\n",
            " -2.3205e-01  1.4030e-01  1.8823e-01 -2.7338e-01  2.3996e-02  1.3699e-03\n",
            "  4.2783e-01 -1.9215e-01  2.3960e-01 -1.0260e-01 -5.3435e-02 -1.3402e-01\n",
            "  4.4034e-01 -3.5863e-01  3.0581e-01  2.4502e-01  6.1903e-01  1.9675e-01\n",
            "  3.1811e-02 -2.4506e-01  8.8145e-02  4.8566e-01 -3.4722e-02  2.7517e-01\n",
            " -2.3461e-01  1.1136e-01  1.2311e-01  8.4186e-03 -3.5173e-01 -2.4092e-01\n",
            " -1.0272e-01 -4.4094e-02 -1.8418e+00  3.7505e-02 -3.2827e-01  9.9435e-03\n",
            " -6.5671e-03 -4.6934e-01 -7.7705e-02  1.2546e-01 -3.0545e-01 -2.7666e-01\n",
            " -6.8074e-02  2.1327e-01  3.2501e-02  6.6648e-02  3.1927e-01  7.8499e-02\n",
            "  2.7441e-02 -2.4934e-01 -1.0803e-03  2.4807e-02  5.1689e-03  6.7919e-02\n",
            " -3.0057e-01  9.9110e-02 -1.0147e-01 -1.3062e-01  4.2887e-01 -2.1415e-01\n",
            "  7.3101e-02 -1.3265e-01 -6.6248e-02 -6.5804e-02  4.0839e-01 -4.4106e-01\n",
            "  4.8157e-01  1.6442e-01 -1.5006e-01 -3.5675e-02 -1.2645e-01  1.1695e-01\n",
            " -1.4322e-01  1.1417e-02 -7.6701e-02 -8.3357e-02  6.4670e-02 -3.0670e-01\n",
            " -3.9173e-01 -3.3955e-01 -6.7373e-02  4.7937e-01 -1.2970e-01 -1.0045e-01\n",
            " -3.2984e-01 -2.4674e-01  4.8181e-01 -8.4101e-02  2.9180e-02 -4.5079e-01\n",
            "  3.5690e-01  2.7968e-01 -1.2863e-01 -3.1201e-01 -2.2766e-01 -5.0461e-01\n",
            "  3.1923e-01  2.3339e-01 -9.2261e-03 -1.6116e-01  2.3816e-01  6.2875e-02\n",
            "  1.3246e-01  1.2601e-01 -1.7466e-01  4.8395e-04  1.2011e-01  2.4306e-01\n",
            " -1.5462e-01 -1.3765e-01 -3.0886e-01 -6.0806e-02  7.1041e-02 -2.8261e-01\n",
            "  1.1564e-01  2.1993e-01  4.0505e-02  1.8259e-01 -1.3313e-01 -8.7736e-02\n",
            " -4.5342e-01 -1.8688e-01 -2.9771e-01  3.2218e-01 -2.7513e-01  4.0793e-01\n",
            " -9.2169e-02  1.4585e-01 -3.0646e-01 -3.3378e-01 -1.3570e-01  3.8484e-01\n",
            " -6.2077e-02  2.5630e-01  3.1604e-01  8.9865e-02  4.3204e-01 -2.1821e-01\n",
            " -2.2809e-01  1.7682e-01 -2.8047e-01  6.8735e-01 -4.1602e-02 -2.1974e-01\n",
            " -5.9114e-01  1.6888e-01  7.1882e-02  2.7301e-01 -5.6362e-02 -2.9558e-01\n",
            " -2.1611e-01  3.1410e-01 -2.0359e-02 -7.0464e-02  7.1177e-02  1.9006e-01\n",
            " -1.9925e-01 -1.8042e-01 -2.4728e-02  4.4026e-01  2.7207e-01  3.1359e-01\n",
            "  1.0982e-01 -2.0591e-01 -2.5455e-01  1.0788e-01 -8.0282e-02  1.7590e-01\n",
            " -3.1330e-02  1.2026e-01 -2.8191e-01  3.0044e-02 -1.0212e-01 -4.2192e-01\n",
            "  1.7524e-02  2.6430e-01  1.3110e-01 -7.5431e-02  2.5701e-01  2.9370e-02\n",
            "  4.9952e-02  1.0670e-01 -1.8945e-01  1.9853e-01 -2.4944e-01  2.7743e-02\n",
            "  4.8727e-01 -2.1255e-01 -4.5050e-01 -1.8632e-01  3.0022e-01  2.7223e-01]\n",
            "[ 3.1924e-01  6.3160e-02 -2.7858e-01  2.6120e-01  7.9248e-02 -2.1462e-01\n",
            " -1.0495e-01  1.5495e-01 -3.3530e-02  2.4834e+00 -5.0904e-01  8.7490e-02\n",
            "  2.1426e-01  2.2151e-01 -2.5234e-01 -9.7544e-02 -1.9270e-01  1.3606e+00\n",
            " -1.1592e-01 -1.0383e-01  2.1929e-01  1.1997e-01 -1.1063e-01  1.4212e-01\n",
            " -1.6643e-01  2.1815e-01  4.2086e-03 -7.0012e-02 -2.3532e-01 -2.6518e-01\n",
            "  3.1248e-02  1.6669e-01 -8.9777e-02  2.0059e-01  3.1614e-01 -5.5830e-01\n",
            "  7.5735e-02  2.7635e-01  1.2741e-01 -1.8185e-01 -1.2722e-01  2.4686e-02\n",
            " -7.7233e-02 -4.8998e-01  2.0355e-02  3.9164e-03  1.2150e-01  8.9723e-02\n",
            " -7.8975e-02  8.1443e-02 -9.9087e-02 -5.5621e-02  1.0737e-01 -4.4042e-03\n",
            "  4.8496e-01  1.1717e-01 -1.7329e-02  1.0900e-01 -3.5558e-01  5.1084e-02\n",
            "  1.5714e-01  1.7961e-01 -2.9711e-01  3.3645e-02 -2.5792e-02 -1.3931e-02\n",
            " -2.3000e-01 -4.0306e-02  2.2282e-01 -1.3544e-02  1.1554e-02  3.9110e-01\n",
            "  2.6533e-01 -3.1012e-01  4.0539e-01 -4.2975e-02  2.0811e-02 -3.3033e-01\n",
            "  1.9573e-01 -3.7958e-02  1.0274e-01 -1.3581e-03 -4.4505e-01  7.7886e-02\n",
            "  8.5110e-02 -2.0285e-01 -1.9481e-01  5.6933e-02  5.3105e-01  3.4154e-02\n",
            " -5.6996e-01 -1.8469e-01  9.3403e-02  2.8044e-01 -2.3349e-01  1.0938e-01\n",
            " -1.4288e-02 -2.7400e-01  3.4196e-02 -9.8479e-02  1.3268e-01  1.9437e-01\n",
            "  1.3463e-01 -9.9059e-02  4.0324e-02 -6.6272e-01  3.5710e-01  1.5429e-01\n",
            "  1.8598e-01  8.7542e-02  8.0538e-02 -2.5121e-01  2.4155e-01  1.7830e-01\n",
            "  3.6011e-02 -2.7677e-02  2.1161e-01 -2.9107e-01 -8.3456e-03  1.1317e-01\n",
            "  3.1064e-01 -1.0693e-01 -2.7367e-01 -3.9785e-02  3.9881e-02  3.4462e-02\n",
            " -1.6518e-01  1.6115e-01  6.0826e-02  3.0750e-01 -2.2398e-01  1.4619e-01\n",
            " -2.6610e-01  4.9732e-01 -1.3996e-01 -2.4287e-01  3.9469e-02 -8.4495e-02\n",
            " -2.4315e-01  7.0701e-02 -1.0136e+00 -2.1733e-01 -3.6878e-01 -2.4973e-01\n",
            "  1.7472e-01 -1.1592e-02  6.8561e-02 -9.0411e-02  2.1878e-01 -2.6390e-01\n",
            "  1.1904e-01  1.4285e-01 -1.8707e-01 -1.3474e-01 -1.3232e-01 -2.6553e-01\n",
            "  2.2947e-01 -1.8215e-02  6.7383e-03 -1.0190e-01  1.0053e-01 -1.1270e-01\n",
            " -1.3295e-01  1.5951e-01  1.4906e-01 -9.5578e-02  2.6992e-01  1.1057e-02\n",
            "  5.6568e-02  2.1386e-02  2.0215e-01  4.8589e-04  5.3360e-01 -2.2947e-01\n",
            "  2.9275e-01  1.7378e-01  2.5423e-01 -1.0976e-01  5.8816e-02  1.4616e-02\n",
            " -4.3060e-02  1.0732e-01 -2.8149e-02 -1.9181e-01  1.0250e-01 -6.3892e-02\n",
            "  1.2737e-02 -1.2913e-01  1.5037e-02  2.6562e-01 -1.7049e-02 -6.0716e-02\n",
            " -9.4919e-02  1.7775e-02  1.3221e-01  1.6830e-01 -1.9323e-01 -1.7612e-01\n",
            "  7.5506e-02  1.8939e-01  1.2508e-01 -1.9880e-01 -1.6017e-01 -2.1092e-01\n",
            "  4.6933e-01  4.4747e-02  9.8349e-02  1.1637e-02  2.2281e-01 -1.0837e-02\n",
            " -4.8330e-02 -4.7335e-01 -3.6811e-01 -1.3592e-01 -1.5086e-01  2.5416e-01\n",
            "  6.9531e-02  1.4211e-01 -2.6703e-01 -1.2590e-01  1.2076e-01 -2.6117e-01\n",
            "  3.3024e-02 -3.4398e-02 -1.3968e-01  1.3446e-01 -1.6709e-01  1.5002e-01\n",
            " -1.3724e-01  9.1226e-02 -2.7718e-01  2.0098e-02  2.6919e-01  4.3016e-01\n",
            "  9.4019e-02 -8.5496e-02 -2.5192e-01 -1.1645e-01 -3.9734e-02  4.6738e-03\n",
            "  5.4178e-01 -1.6636e-01  3.4546e-01  9.8501e-02  4.7819e-01 -3.8428e-01\n",
            " -3.2380e-01 -1.4822e-01 -4.7817e-01  1.6704e-01 -6.4505e-02  1.1834e-01\n",
            " -3.4480e-01  9.6891e-02  3.2309e-01  4.1471e-01  1.9463e-01 -2.0891e-01\n",
            " -1.2223e-01 -5.8298e-02 -2.0268e-01  2.9480e-01  4.3397e-02  1.0112e-01\n",
            "  2.7177e-01 -5.2124e-01 -7.3794e-02  4.4808e-02  4.1388e-01  8.8782e-02\n",
            "  6.2255e-01 -7.2391e-02  9.0129e-02  1.5428e-01  2.3163e-02 -1.3028e-01\n",
            "  6.1762e-02  3.3803e-01 -9.1581e-02  2.1039e-01  5.1080e-02  1.9184e-01\n",
            "  1.0444e-01  2.1380e-01 -3.5091e-01 -2.3702e-01  3.8399e-02 -1.0031e-01\n",
            "  1.8359e-01  2.5178e-02 -1.2977e-01  3.7130e-01  1.8888e-01 -4.2738e-03\n",
            " -1.0645e-01 -2.5810e-01 -4.4629e-02  8.2745e-02  9.7801e-02  2.5045e-01]\n",
            "[ 1.3893e-01 -1.9056e-02 -3.3891e-01  1.2151e-01  3.6523e-01 -1.7391e-01\n",
            " -2.6735e-02 -5.0335e-02  2.4743e-01  2.4531e+00 -4.2113e-01  2.3632e-01\n",
            "  2.0513e-01 -1.0937e-02 -1.1480e-01 -3.7648e-02 -1.3440e-01  8.6124e-01\n",
            " -3.5803e-01  9.2525e-02  2.8075e-01  1.3649e-01  2.0819e-01  6.0206e-02\n",
            " -1.8229e-01  1.0172e-01 -1.3200e-01 -3.1598e-01  2.2241e-01 -1.9076e-01\n",
            " -1.0884e-02  1.6988e-01  8.0345e-03  1.3337e-01  1.7724e-01 -1.9162e-01\n",
            "  3.3681e-01  3.0186e-01  6.1654e-02  7.6906e-03 -5.4406e-01  5.0142e-02\n",
            " -4.3115e-02 -2.6241e-01  4.7462e-02  3.3670e-01 -2.8649e-01 -2.7414e-01\n",
            "  2.6776e-02 -6.5939e-02  1.1021e-01  2.8869e-01  4.6712e-01  1.2063e-01\n",
            "  3.3831e-01 -3.0427e-04 -1.2116e-01 -1.5900e-01 -1.0514e-01 -3.8560e-02\n",
            " -6.2205e-02  3.5631e-02 -1.7852e-01 -1.3308e-01  2.6103e-01 -1.1082e-01\n",
            " -2.7463e-01  1.8556e-01  4.5257e-01  3.0336e-01  6.1801e-02  7.7310e-02\n",
            "  3.4645e-01  3.6526e-03  4.6815e-01  2.0228e-02 -2.5509e-02 -1.9465e-02\n",
            " -5.3998e-03  8.6497e-02 -5.3099e-02 -8.6426e-02 -4.6913e-01  6.5788e-02\n",
            " -1.2720e-01 -2.4254e-01  2.4149e-01 -3.7684e-01  6.5707e-01  1.4106e-01\n",
            " -2.1080e-01  1.1095e-01  1.2741e-01 -2.8938e-01 -7.5295e-02  4.4109e-02\n",
            "  5.4280e-02 -2.9666e-01  1.6423e-02  4.4086e-02  7.2862e-02 -3.0149e-01\n",
            "  3.4613e-02  8.6731e-02  3.1091e-01 -3.7156e-01  1.7382e-01  2.1996e-01\n",
            "  6.0312e-02  1.6767e-01  3.8469e-02 -6.3231e-01  3.2331e-01  1.0421e-01\n",
            "  2.5080e-01  2.6267e-01  1.7882e-01 -2.4515e-01 -1.7214e-01  2.8319e-01\n",
            "  6.5598e-02 -4.4386e-02  3.7064e-02  4.5018e-02  3.9367e-01 -1.3580e-01\n",
            " -4.6021e-02  2.3260e-01  3.4156e-02  8.4838e-02  3.8056e-02 -3.4659e-03\n",
            " -6.8177e-02  8.2606e-02  1.5811e-01 -1.9388e-01  6.1247e-02 -3.4282e-01\n",
            " -2.3392e-01  9.1131e-02 -2.0934e+00  1.3815e-01  1.7597e-01 -3.8005e-01\n",
            "  2.4690e-01 -2.2482e-01 -5.9415e-02  4.2415e-01 -1.3768e-01 -6.4184e-02\n",
            " -6.3230e-02  1.0228e-01  3.9662e-02 -2.8910e-01  8.0242e-02 -4.2854e-01\n",
            "  1.0474e-01 -1.4070e-01 -5.0420e-02 -1.3486e-01 -1.3077e-01 -1.1560e-01\n",
            " -3.7648e-01  1.4467e-02  2.8400e-02 -1.4921e-01  2.9029e-01 -9.9163e-02\n",
            "  2.2967e-01  4.9717e-02 -2.4345e-01  2.7722e-02  7.4055e-02 -4.2579e-01\n",
            "  1.6724e-01  6.5469e-02  7.7571e-02  2.1529e-01 -7.1330e-02 -5.2742e-02\n",
            "  1.2080e-01 -5.6503e-02 -3.5852e-01 -8.5641e-02  5.3364e-01  1.8189e-01\n",
            " -1.7887e-01 -2.0100e-01  4.3655e-01  1.7912e-01  2.2238e-02 -4.8491e-02\n",
            " -8.8347e-02 -1.7544e-01 -1.7052e-01  2.7578e-01 -2.7629e-01 -5.6030e-02\n",
            "  3.0416e-01  4.5115e-01  6.8440e-02 -1.5926e-01 -2.7197e-01 -2.2628e-02\n",
            "  3.4567e-01 -8.8622e-02  1.9358e-01 -4.1653e-02  4.1661e-01  1.0848e-01\n",
            " -6.9752e-02 -2.3749e-01 -1.8511e-01  8.8535e-03 -1.2773e-01 -8.2802e-02\n",
            "  4.5794e-02  3.3554e-01 -3.0644e-01  4.4171e-01  1.5975e-01 -1.2756e-02\n",
            " -2.6972e-01 -7.8977e-02 -4.8264e-02 -6.5962e-02 -3.0006e-01  1.0440e-01\n",
            " -1.2715e-01 -1.4488e-02 -3.5107e-01 -1.1407e-01  6.6343e-01  1.6589e-01\n",
            "  1.5040e-02 -8.3249e-03  1.3835e-02 -1.9600e-01 -3.6130e-02  2.0337e-01\n",
            "  1.1822e-01  1.4760e-01 -3.9074e-02  2.4619e-01 -9.6280e-02 -2.2719e-01\n",
            " -2.2597e-01  1.9236e-01 -4.1362e-01  3.7821e-01  2.8114e-01 -7.8232e-02\n",
            " -2.4852e-01  8.4978e-02  4.5680e-01  4.7818e-01 -7.2192e-02 -2.2441e-01\n",
            " -2.5713e-01  1.1820e-01  1.8028e-01  5.1181e-01  4.0904e-01 -2.4304e-01\n",
            "  4.9090e-01 -3.9707e-01 -5.1963e-02  1.7818e-02  2.5160e-01  2.8290e-01\n",
            "  2.2776e-01 -2.5844e-01  1.4281e-01 -7.2607e-02  1.5194e-01  2.5309e-01\n",
            "  4.7232e-02  2.5905e-01 -9.3313e-02  1.1226e-01 -1.7449e-01 -3.4896e-01\n",
            " -2.0145e-01  1.1628e-01 -1.0769e-01 -7.8528e-02  9.3096e-02 -1.6539e-01\n",
            "  4.3994e-02  5.9698e-02 -1.3047e-01  7.2147e-02  3.3663e-03 -1.8181e-01\n",
            "  3.1465e-02 -3.5351e-02 -4.7912e-03  9.2753e-02  2.8618e-01  1.3646e-01]\n",
            "[ 3.1924e-01  6.3160e-02 -2.7858e-01  2.6120e-01  7.9248e-02 -2.1462e-01\n",
            " -1.0495e-01  1.5495e-01 -3.3530e-02  2.4834e+00 -5.0904e-01  8.7490e-02\n",
            "  2.1426e-01  2.2151e-01 -2.5234e-01 -9.7544e-02 -1.9270e-01  1.3606e+00\n",
            " -1.1592e-01 -1.0383e-01  2.1929e-01  1.1997e-01 -1.1063e-01  1.4212e-01\n",
            " -1.6643e-01  2.1815e-01  4.2086e-03 -7.0012e-02 -2.3532e-01 -2.6518e-01\n",
            "  3.1248e-02  1.6669e-01 -8.9777e-02  2.0059e-01  3.1614e-01 -5.5830e-01\n",
            "  7.5735e-02  2.7635e-01  1.2741e-01 -1.8185e-01 -1.2722e-01  2.4686e-02\n",
            " -7.7233e-02 -4.8998e-01  2.0355e-02  3.9164e-03  1.2150e-01  8.9723e-02\n",
            " -7.8975e-02  8.1443e-02 -9.9087e-02 -5.5621e-02  1.0737e-01 -4.4042e-03\n",
            "  4.8496e-01  1.1717e-01 -1.7329e-02  1.0900e-01 -3.5558e-01  5.1084e-02\n",
            "  1.5714e-01  1.7961e-01 -2.9711e-01  3.3645e-02 -2.5792e-02 -1.3931e-02\n",
            " -2.3000e-01 -4.0306e-02  2.2282e-01 -1.3544e-02  1.1554e-02  3.9110e-01\n",
            "  2.6533e-01 -3.1012e-01  4.0539e-01 -4.2975e-02  2.0811e-02 -3.3033e-01\n",
            "  1.9573e-01 -3.7958e-02  1.0274e-01 -1.3581e-03 -4.4505e-01  7.7886e-02\n",
            "  8.5110e-02 -2.0285e-01 -1.9481e-01  5.6933e-02  5.3105e-01  3.4154e-02\n",
            " -5.6996e-01 -1.8469e-01  9.3403e-02  2.8044e-01 -2.3349e-01  1.0938e-01\n",
            " -1.4288e-02 -2.7400e-01  3.4196e-02 -9.8479e-02  1.3268e-01  1.9437e-01\n",
            "  1.3463e-01 -9.9059e-02  4.0324e-02 -6.6272e-01  3.5710e-01  1.5429e-01\n",
            "  1.8598e-01  8.7542e-02  8.0538e-02 -2.5121e-01  2.4155e-01  1.7830e-01\n",
            "  3.6011e-02 -2.7677e-02  2.1161e-01 -2.9107e-01 -8.3456e-03  1.1317e-01\n",
            "  3.1064e-01 -1.0693e-01 -2.7367e-01 -3.9785e-02  3.9881e-02  3.4462e-02\n",
            " -1.6518e-01  1.6115e-01  6.0826e-02  3.0750e-01 -2.2398e-01  1.4619e-01\n",
            " -2.6610e-01  4.9732e-01 -1.3996e-01 -2.4287e-01  3.9469e-02 -8.4495e-02\n",
            " -2.4315e-01  7.0701e-02 -1.0136e+00 -2.1733e-01 -3.6878e-01 -2.4973e-01\n",
            "  1.7472e-01 -1.1592e-02  6.8561e-02 -9.0411e-02  2.1878e-01 -2.6390e-01\n",
            "  1.1904e-01  1.4285e-01 -1.8707e-01 -1.3474e-01 -1.3232e-01 -2.6553e-01\n",
            "  2.2947e-01 -1.8215e-02  6.7383e-03 -1.0190e-01  1.0053e-01 -1.1270e-01\n",
            " -1.3295e-01  1.5951e-01  1.4906e-01 -9.5578e-02  2.6992e-01  1.1057e-02\n",
            "  5.6568e-02  2.1386e-02  2.0215e-01  4.8589e-04  5.3360e-01 -2.2947e-01\n",
            "  2.9275e-01  1.7378e-01  2.5423e-01 -1.0976e-01  5.8816e-02  1.4616e-02\n",
            " -4.3060e-02  1.0732e-01 -2.8149e-02 -1.9181e-01  1.0250e-01 -6.3892e-02\n",
            "  1.2737e-02 -1.2913e-01  1.5037e-02  2.6562e-01 -1.7049e-02 -6.0716e-02\n",
            " -9.4919e-02  1.7775e-02  1.3221e-01  1.6830e-01 -1.9323e-01 -1.7612e-01\n",
            "  7.5506e-02  1.8939e-01  1.2508e-01 -1.9880e-01 -1.6017e-01 -2.1092e-01\n",
            "  4.6933e-01  4.4747e-02  9.8349e-02  1.1637e-02  2.2281e-01 -1.0837e-02\n",
            " -4.8330e-02 -4.7335e-01 -3.6811e-01 -1.3592e-01 -1.5086e-01  2.5416e-01\n",
            "  6.9531e-02  1.4211e-01 -2.6703e-01 -1.2590e-01  1.2076e-01 -2.6117e-01\n",
            "  3.3024e-02 -3.4398e-02 -1.3968e-01  1.3446e-01 -1.6709e-01  1.5002e-01\n",
            " -1.3724e-01  9.1226e-02 -2.7718e-01  2.0098e-02  2.6919e-01  4.3016e-01\n",
            "  9.4019e-02 -8.5496e-02 -2.5192e-01 -1.1645e-01 -3.9734e-02  4.6738e-03\n",
            "  5.4178e-01 -1.6636e-01  3.4546e-01  9.8501e-02  4.7819e-01 -3.8428e-01\n",
            " -3.2380e-01 -1.4822e-01 -4.7817e-01  1.6704e-01 -6.4505e-02  1.1834e-01\n",
            " -3.4480e-01  9.6891e-02  3.2309e-01  4.1471e-01  1.9463e-01 -2.0891e-01\n",
            " -1.2223e-01 -5.8298e-02 -2.0268e-01  2.9480e-01  4.3397e-02  1.0112e-01\n",
            "  2.7177e-01 -5.2124e-01 -7.3794e-02  4.4808e-02  4.1388e-01  8.8782e-02\n",
            "  6.2255e-01 -7.2391e-02  9.0129e-02  1.5428e-01  2.3163e-02 -1.3028e-01\n",
            "  6.1762e-02  3.3803e-01 -9.1581e-02  2.1039e-01  5.1080e-02  1.9184e-01\n",
            "  1.0444e-01  2.1380e-01 -3.5091e-01 -2.3702e-01  3.8399e-02 -1.0031e-01\n",
            "  1.8359e-01  2.5178e-02 -1.2977e-01  3.7130e-01  1.8888e-01 -4.2738e-03\n",
            " -1.0645e-01 -2.5810e-01 -4.4629e-02  8.2745e-02  9.7801e-02  2.5045e-01]\n",
            "[ 0.25885    0.41669    0.012939  -0.3022    -0.2388    -0.45782\n",
            " -0.40049   -0.47336    0.25669   -0.75997   -0.50078   -0.36793\n",
            "  0.25197   -0.17929    0.11294   -0.19626   -0.15453   -1.5194\n",
            " -0.9219    -0.16021    0.49491    0.37269   -0.22884    0.062661\n",
            "  0.039548  -0.56158   -1.0188    -0.59943    0.19722    0.089292\n",
            " -0.81983    0.26819   -0.025854   0.17347   -0.054767  -0.09972\n",
            " -0.36587    0.11922   -0.52245   -0.35914   -0.12046    0.12854\n",
            "  0.42416    0.29781    0.73514    0.16669   -0.1809    -0.35014\n",
            "  0.21949    0.089572   0.20576   -0.48113    0.31733   -0.39014\n",
            "  0.39798    0.14191    0.021055  -0.55106    0.010946  -0.21032\n",
            "  0.052764   0.32341   -0.20387   -0.80243    0.38031    0.17698\n",
            " -0.7562     0.11175   -0.22624   -0.18735    0.36163   -0.038567\n",
            " -0.25132   -0.26762    0.0078969 -0.3897     0.11084    0.10213\n",
            "  0.18272    0.086586   0.009115   0.40436    0.64264    0.47688\n",
            "  0.38246    0.04043    1.468     -0.82244   -0.91875   -0.32002\n",
            " -0.29286   -0.24525   -0.24931    0.152     -0.48782    0.18621\n",
            " -0.25888   -0.30609   -0.32241   -0.31894   -0.054701  -0.27621\n",
            " -0.25604   -0.095366  -0.1301    -1.0032     0.098079   0.22059\n",
            "  0.25932   -0.77209   -0.21377    0.35217   -0.53356   -0.85884\n",
            "  0.22923    0.7721    -0.02262    0.31662   -0.59245   -0.15485\n",
            " -0.37656    0.10522    0.488      0.777     -0.78358   -0.10545\n",
            " -0.49901   -0.45147   -0.19045   -0.054088  -0.16374    0.10738\n",
            "  0.20392   -0.081476  -0.017535   0.025854   0.0095385  0.40816\n",
            " -0.22756    0.61361   -0.64077    0.26167   -0.22742    0.59827\n",
            " -0.024887   0.40962   -0.25366   -0.56008   -0.81147    0.32126\n",
            " -0.23674   -0.21975    0.12164    0.25962    1.0671     0.24252\n",
            "  0.035534  -0.33672   -0.33834    0.44516    0.1075    -0.49519\n",
            " -0.36386   -0.30506    1.3294    -0.40892   -0.93228    0.31008\n",
            " -0.60001   -0.063871   0.031057  -0.084458   0.50124    0.33127\n",
            " -0.45182    0.12456    0.15833   -0.10691    0.071651  -0.56059\n",
            "  0.0071474 -0.70444   -0.18484    0.097794   0.27138    0.0938\n",
            "  0.30929    0.41732    0.2427    -0.30812    0.047641   0.76841\n",
            " -0.68788   -0.25946    0.998     -0.17147    0.22369    0.2674\n",
            "  0.044647  -0.30326    0.084101  -0.21845    0.39128   -0.59681\n",
            " -0.23014    0.29604    0.33689   -0.19972   -0.33947   -0.3733\n",
            " -0.185      0.75624   -0.23049   -0.30929    0.80723   -0.2105\n",
            "  0.52084   -0.15608    0.73681    0.52553   -0.39348    0.30274\n",
            "  0.42259   -0.43118   -0.18553    0.34051   -0.65727   -0.19823\n",
            " -0.08557   -0.65784   -0.81053    0.20402   -0.086152   0.017174\n",
            " -1.2962    -0.21197    0.18864   -0.26763    0.46762   -0.10224\n",
            "  0.26375   -0.18106   -0.74019   -0.43821    0.46784    0.28862\n",
            " -0.15327    0.64798    0.52058    0.1778     0.29531   -0.50739\n",
            "  0.20698    0.22463    0.48096   -0.41559   -0.13676   -0.23959\n",
            " -0.31646   -0.71865    0.01812    0.074746   0.36952    0.17299\n",
            "  0.52479   -0.37977   -0.35208    0.15917    0.015063   0.52023\n",
            " -0.57755   -0.34366   -0.31014    0.1972    -0.77024    0.37305\n",
            "  0.18602    0.64642    0.11752   -0.16545   -0.096619   0.20849\n",
            "  0.048231  -0.064352   0.4677     0.84811    0.54414   -0.21618\n",
            " -0.31459   -0.5808     0.036628   0.28875   -0.19005    0.13418\n",
            "  0.28884    0.27628    0.20277    0.43841    0.41145    0.43155  ]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHumnrdF0-sN"
      },
      "source": [
        "You can notice that when vector is not present for a token, the value of vector_norm is 0 for it.\n",
        "\n",
        "Identifying similarity of two words or tokens is very crucial . It is the base to many everyday NLP tasks like text classification , recommendation systems, etc.. It is necessary to know how similar two sentences are , so they can be grouped in same or opposite category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT1n_5K01A1P"
      },
      "source": [
        "How to find similarity of two tokens?\n",
        "\n",
        "Every Doc or Token object has the function similarity(), using which you can compare it with another doc or token.\n",
        "\n",
        "Know about cosine similarity.\n",
        "\n",
        "It returns a float value. Higher the value is, more similar are the two tokens or documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dozEg3h04nF",
        "outputId": "39bfa45f-5f10-4d2e-88b3-dc4fc6f2bcce"
      },
      "source": [
        "# Compute Similarity\n",
        "token_1=nlp(\"I am a software engineer.\")\n",
        "token_2=nlp(\"I work with software.\")\n",
        "\n",
        "similarity_score= token_1.similarity(token_2)\n",
        "print(similarity_score)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8923226154433829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7810Bi4S1Fm_"
      },
      "source": [
        "That is how you use the similarity function.\n",
        "\n",
        "Let me show you an example of how similarity() function on docs can help in text categorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfi160f-1Ci_",
        "outputId": "20ce6044-a2c3-4adc-c06a-5e7b6b4fc1ac"
      },
      "source": [
        "review_1=nlp(' The food was amazing')\n",
        "review_2=nlp('The food was excellent')\n",
        "review_3=nlp('I did not like the food')\n",
        "review_4=nlp('It was very bad experience')\n",
        "\n",
        "score_1=review_1.similarity(review_2)\n",
        "print('Similarity between review 1 and 2',score_1)\n",
        "\n",
        "score_2=review_3.similarity(review_4)\n",
        "print('Similarity between review 3 and 4',score_2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity between review 1 and 2 0.9566212627033192\n",
            "Similarity between review 3 and 4 0.8461898618188776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr3Tm1x21Kj8"
      },
      "source": [
        "You can see that first two reviews have high similarity score and hence will belong in the same category(positive).\n",
        "\n",
        "You can also check if two tokens or docs are related (includes both similar side and opposite sides) or completely irrelevant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAKzvryM1HWR",
        "outputId": "8e233d99-0fd3-450e-ac37-9cef4707d109"
      },
      "source": [
        "# Compute Similarity between texts \n",
        "pizza=nlp('pizza')\n",
        "burger=nlp('burger')\n",
        "chair=nlp('chair')\n",
        "\n",
        "print('Pizza and burger  ',pizza.similarity(burger))\n",
        "print('Pizza and chair  ',pizza.similarity(chair))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pizza and burger   0.7269758865234512\n",
            "Pizza and chair   0.1917966191121549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdGcIqmI1RpX"
      },
      "source": [
        "You can observe that pizza and burger are both food items and have good similarity score.\n",
        "\n",
        "Whereas, pizza and chair are completely irrelevant and score is very low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdIUxp4K1T_8"
      },
      "source": [
        "You have used tokens and docs in many ways till now. In this section, let’s dive deeper and understand the basic pipeline behind this.\n",
        "\n",
        "When you call the nlp object on spaCy, the text is segmented into tokens to create a Doc object. Following this, various process are carried out on the Doc to add the attributes like POS tags, Lemma tags, dependency tags,etc..\n",
        "\n",
        "This is referred as the Processing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfHzM4ly1XVX"
      },
      "source": [
        "The processing pipeline consists of components, where each component performs it’s task and passes the Processed Doc to the next component. These are called as pipeline components.\n",
        "\n",
        "spaCy provides certain in-built pipeline components. Let’s look at them.\n",
        "\n",
        "The built-in pipeline components of spacy are :\n",
        "\n",
        "Tokenizer : It is responsible for segmenting the text into tokens are turning a Doc object. This the first and compulsory step in a pipeline.\n",
        "Tagger : It is responsible for assigning Part-of-speech tags. It takes a Doc as input and createsDoc[i].tag\n",
        "\n",
        "DependencyParser : It is known as parser. It is responsible for assigning the dependency tags to each token. It takes a Doc as input and returns the processed Doc\n",
        "\n",
        "EntityRecognizer : This component is referred as ner. It is responsible for identifying named entities and assigning labels to them.\n",
        "\n",
        "TextCategorizer : This component is called textcat. It will assign categories to Docs.\n",
        "\n",
        "EntityRuler : This component is called * entity_ruler*.It is responsible for assigning named entitile based on pattern rules. Revisit Rule Based Matching to know more.\n",
        "\n",
        "Sentencizer : This component is called **sentencizer** and can perform rule based sentence segmentation.\n",
        "\n",
        "merge_noun_chunks : It is called mergenounchunks. This component is responsible for merging all noun chunks into a single token. It has to be add in the pipeline after tagger and parser.\n",
        "\n",
        "merge_entities : It is called merge_entities .This component can merge all entities into a single token. It has to added after the ner.\n",
        "\n",
        "merge_subtokens : It is called merge_subtokens. This component can merge the subtokens into a single token.\n",
        "\n",
        "These are the various in-built pipeline components. It is not necessary for every spaCy model to have each of the above components.\n",
        "\n",
        "After loading a spaCy model , you check or inspect what pipeline components are present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMSj_h2D1aTE"
      },
      "source": [
        "After loading the spacy model and creating a Language object nlp, you view the list of pipeline components present by default using nlp.pipe_names attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN91JTcr1MXo",
        "outputId": "c2a18d62-5994-4970-b019-989ce551ed5c"
      },
      "source": [
        "# Inspect a pipeline\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "print(nlp.pipe_names)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQpXsaFu1dwc"
      },
      "source": [
        "You can also check if a particular component is present in the pipline through nlp.has_pipe. You have to pass the name of the component like tagger , ner ,textcat as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2dklWI-1cAH",
        "outputId": "a86e6f10-a82f-41a5-801b-eb75473482f7"
      },
      "source": [
        "# Check if pipeline component present\n",
        "nlp.has_pipe('textcat')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7VAHC0x1iV6"
      },
      "source": [
        "## **Customize Spacy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlVI1PSB1odK"
      },
      "source": [
        "You can add a component to the processing pipeline through nlp.add_pipe() method. You have to pass the component to be added as input.\n",
        "\n",
        "The component can also be written by you, i.e, custom made pipeline component. (We will come to this later). In case you want to add an in-built component like textcat, how to do it ?\n",
        "\n",
        "You can use nlp.create_pipe() and pass the component name to get any in-built pipeline component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDqIH3tU1fwN"
      },
      "source": [
        "# Add new pipeline component\n",
        "nlp.add_pipe(nlp.create_pipe('textcat'))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vlroIWo1r-J"
      },
      "source": [
        "Now , you can verify if the component was added using nlp.pipe_names()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO4z17bn1qDO",
        "outputId": "5330897f-80b5-4fb4-ec45-e336f5ccfe01"
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner', 'textcat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWJzGLt_1xZz"
      },
      "source": [
        "Observe that textcat has been added at the last. The order of the components signify the order in which the Doc will be processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6HhfGnx1zpM"
      },
      "source": [
        "How to specify where you want to add the new component?\n",
        "\n",
        "The nlp.add_pipe() method provides various arguments for this. You can set one among before, after, first or last to True.\n",
        "\n",
        "By default, last=True is used.\n",
        "\n",
        "If you want textcat before ner, you can set before=ner. If you want it to be at first you can set first=True. Just remeber that you should not pass more than one of these arguments as it will lead to contradiction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Itl-D1-11w6A",
        "outputId": "a4644b4e-2660-4ba9-b47c-0ab9294e6a37"
      },
      "source": [
        "# Adding a pipeline component\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(nlp.create_pipe('textcat'),before='ner')\n",
        "nlp.pipe_names"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'textcat', 'ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8mAaW_v11RK",
        "outputId": "e53002c2-54c6-4bda-b8c5-76d164bcf71a"
      },
      "source": [
        "# Removing a pipeline component and printing \n",
        "nlp.remove_pipe(\"textcat\")\n",
        "print('After removing the textcat pipeline')\n",
        "print(nlp.pipe_names)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After removing the textcat pipeline\n",
            "['tagger', 'parser', 'ner']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwJxaOdn19UH",
        "outputId": "cd69307a-3098-4360-cc1b-1846d9a344dc"
      },
      "source": [
        "# Renaming pipeline components\n",
        "nlp.rename_pipe(old_name='ner',new_name='my_custom_ner')\n",
        "nlp.pipe_names"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'my_custom_ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipxH4QJv2Xnm"
      },
      "source": [
        "# Increasing efficiency of Spacy pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oWRwg-q2x0w"
      },
      "source": [
        "While dealing with huge amount of text data , the process of converting the text into processed Doc ( passing through pipeline components) is often time consuming.\n",
        "\n",
        "In this section , you’ll learn various methods for different situations to help you reduce computational expense.\n",
        "\n",
        "Let’s say you have a list of text data , and you want to process them into Doc onject. The traditional method is to call nlp object on each of the text data . Below is the given list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLNWORJj2ob6"
      },
      "source": [
        "list_of_text_data=['In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.','Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.','Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving','As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect.','The term military simulation can cover a wide spectrum of activities, ranging from full-scale field-exercises,[2] to abstract computerized models that can proceed with little or no human involvement','As a general scientific principle, the most reliable data comes from actual observation and the most reliable theories depend on it.[4] This also holds true in military analysis','Any form of training can be regarded as a \"simulation\" in the strictest sense of the word (inasmuch as it simulates an operational environment); however, many if not most exercises take place not to test new ideas or models, but to provide the participants with the skills to operate within existing ones.','ull-scale military exercises, or even smaller-scale ones, are not always feasible or even desirable. Availability of resources, including money, is a significant factor—it costs a lot to release troops and materiel from any standing commitments, to transport them to a suitable location, and then to cover additional expenses such as petroleum, oil and lubricants (POL) usage, equipment maintenance, supplies and consumables replenishment and other items','Moving away from the field exercise, it is often more convenient to test a theory by reducing the level of personnel involvement. Map exercises can be conducted involving senior officers and planners, but without the need to physically move around any troops. These retain some human input, and thus can still reflect to some extent the human imponderables that make warfare so challenging to model, with the advantage of reduced costs and increased accessibility. A map exercise can also be conducted with far less forward planning than a full-scale deployment, making it an attractive option for more minor simulations that would not merit anything larger, as well as for very major operations where cost, or secrecy, is an issue']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Ppag8w21UI"
      },
      "source": [
        "First , create the doc normally calling nlp() on each individual text. You can use %%timeit to know the time taken.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEpvlGih1_nJ",
        "outputId": "5b342d1d-c7f9-47f4-c36c-5de467b29d61"
      },
      "source": [
        "%%timeit\n",
        "docs = [nlp(text) for text in list_of_text_data]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 477 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mNzbyh_2byC"
      },
      "source": [
        "You can observe the time taken. Another efficient method of creating the doc is using nlp.pipe() method. You can pass the list as input to this. This method takes less time , as it processes the texts as a stream rather than individually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9x5UGqc2hBb",
        "outputId": "676163da-6c10-49d8-8dd3-f9e556641a30"
      },
      "source": [
        "%%timeit\n",
        "docs = list(nlp.pipe(list_of_text_data))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 321 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzTNJ-jn26HL"
      },
      "source": [
        "From above output , you can observe that time taken is less using nlp.pipe() method. When the amount of data will be very large, the time difference will be very important.\n",
        "\n",
        "Another way to keep the process efficient is using only the pipeline components you need. For example , if your problem does not use POS tags , then tagger is not necessary.\n",
        "\n",
        "The unnecessary pipeline components can be disabled to improve loading speed and efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBocwE_G2-B7"
      },
      "source": [
        "There are two common cases where you will need to disable pipeline components.\n",
        "\n",
        "First case is when you don’t need the component throughout your project. In this case, you can disable the component while loading the spacy model itself. This will save you a great deal of time. It can be done through the disable argument of spacy.load() function.\n",
        "\n",
        "Below code demonstrates how to disable loading of tagger and parser.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzb9A9Dw2qCa",
        "outputId": "796ef7e0-ebc2-4469-9aab-e702a8843702"
      },
      "source": [
        "nlp=spacy.load('en_core_web_sm')\n",
        "for doc in nlp.pipe(list_of_text_data, disable=[\"ner\", \"parser\"]):\n",
        "  print(doc.is_tagged)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1tMyQyh3sWD"
      },
      "source": [
        "# How to Train spaCy to Autodetect New Entities (NER)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3cMwrjo3BUk",
        "outputId": "5e5a6240-bf7e-47a1-fe52-c970dd5d6555"
      },
      "source": [
        "# Performing NER on E-commerce article\n",
        "\n",
        "article_text=\"\"\"India that previously comprised only a handful of players in the e-commerce space, is now home to many biggies and giants battling out with each other to reach the top. This is thanks to the overwhelming internet and smartphone penetration coupled with the ever-increasing digital adoption across the country. These new-age innovations not only gave emerging startups a unique platform to deliver seamless shopping experiences but also provided brick and mortar stores with a level-playing field to begin their online journeys without leaving their offline legacies.\n",
        "In the wake of so many players coming together on one platform, the Indian e-commerce market is envisioned to reach USD 84 billion in 2021 from USD 24 billion in 2017. Further, with the rate at which internet penetration is increasing, we can expect more and more international retailers coming to India in addition to a large pool of new startups. This, in turn, will provide a major Philip to the organized retail market and boost its share from 12% in 2017 to 22-25% by 2021. \n",
        "Here’s a view to the e-commerce giants that are dominating India’s online shopping space:\n",
        "Amazon – One of the uncontested global leaders, Amazon started its journey as a simple online bookstore that gradually expanded its reach to provide a large suite of diversified products including media, furniture, food, and electronics, among others. And now with the launch of Amazon Prime and Amazon Music Limited, it has taken customer experience to a godly level, which will remain undefeatable for a very long time. \n",
        "Flipkart – Founded in 2007, Flipkart is recognized as the national leader in the Indian e-commerce market. Just like Amazon, it started operating by selling books and then entered other categories such as electronics, fashion, and lifestyle, mobile phones, etc. And now that it has been acquired by Walmart, one of the largest leading platforms of e-commerce in the US, it has also raised its bar of customer offerings in all aspects and giving huge competition to Amazon. \n",
        "Snapdeal – Started as a daily deals platform in 2010, Snapdeal became a full-fledged online marketplace in 2011 comprising more than 3 lac sellers across India. The platform offers over 30 million products across 800+ diverse categories from over 125,000 regional, national, and international brands and retailers. The Indian e-commerce firm follows a robust strategy to stay at the forefront of innovation and deliver seamless customer offerings to its wide customer base. It has shown great potential for recovery in recent years despite losing Freecharge and Unicommerce. \n",
        "ShopClues – Another renowned name in the Indian e-commerce industry, ShopClues was founded in July 2011. It’s a Gurugram based company having a current valuation of INR 1.1 billion and is backed by prominent names including Nexus Venture Partners, Tiger Global, and Helion Ventures as its major investors. Presently, the platform comprises more than 5 lac sellers selling products in nine different categories such as computers, cameras, mobiles, etc. \n",
        "Paytm Mall – To compete with the existing e-commerce giants, Paytm, an online payment system has also launched its online marketplace – Paytm Mall, which offers a wide array of products ranging from men and women fashion to groceries and cosmetics, electronics and home products, and many more. The unique thing about this platform is that it serves as a medium for third parties to sell their products directly through the widely-known app – Paytm. \n",
        "Reliance Retail – Given Reliance Jio’s disruptive venture in the Indian telecom space along with a solid market presence of Reliance, it is no wonder that Reliance will soon be foraying into retail space. As of now, it has plans to build an e-commerce space that will be established on online-to-offline market program and aim to bring local merchants on board to help them boost their sales and compete with the existing industry leaders. \n",
        "Big Basket – India’s biggest online supermarket, Big Basket provides a wide variety of imported and gourmet products through two types of delivery services – express delivery and slotted delivery. It also offers pre-cut fruits along with a long list of beverages including fresh juices, cold drinks, hot teas, etc. Moreover, it not only provides farm-fresh products but also ensures that the farmer gets better prices. \n",
        "Grofers – One of the leading e-commerce players in the grocery segment, Grofers started its operations in 2013 and has reached overwhelming heights in the last 5 years. Its wide range of products includes atta, milk, oil, daily need products, vegetables, dairy products, juices, beverages, among others. With its growing reach across India, it has become one of the favorite supermarkets for Indian consumers who want to shop grocery items from the comforts of their homes. \n",
        "Digital Mall of Asia – Going live in 2020, Digital Mall of Asia is a very unique concept coined by the founders of Yokeasia Malls. It is designed to provide an immersive digital space equipped with multiple visual and sensory elements to sellers and shoppers. It will also give retailers exclusive rights to sell a particular product category or brand in their respective cities. What makes it unique is its zero-commission model enabling retailers to pay only a fixed amount of monthly rental instead of paying commissions. With its one-of-a-kind features, DMA is expected to bring\n",
        "never-seen transformation to the current e-commerce ecosystem while addressing all the existing e-commerce worries such as counterfeiting. \"\"\"\n",
        "\n",
        "doc=nlp(article_text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text,ent.label_)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "India GPE\n",
            "one CARDINAL\n",
            "Indian NORP\n",
            "USD 84 billion MONEY\n",
            "2021 DATE\n",
            "USD 24 billion MONEY\n",
            "2017 DATE\n",
            "India GPE\n",
            "Philip PERSON\n",
            "12% PERCENT\n",
            "2017 DATE\n",
            "22-25% PERCENT\n",
            "2021 DATE\n",
            "India GPE\n",
            "Amazon ORG\n",
            "One CARDINAL\n",
            "Amazon ORG\n",
            "Amazon ORG\n",
            "Amazon Music Limited ORG\n",
            "Flipkart PERSON\n",
            "2007 DATE\n",
            "Flipkart PERSON\n",
            "Indian NORP\n",
            "Amazon ORG\n",
            "Walmart LOC\n",
            "one CARDINAL\n",
            "US GPE\n",
            "Amazon ORG\n",
            "daily DATE\n",
            "2010 DATE\n",
            "2011 DATE\n",
            "more than 3 CARDINAL\n",
            "India GPE\n",
            "over 30 million CARDINAL\n",
            "over 125,000 CARDINAL\n",
            "Indian NORP\n",
            "recent years DATE\n",
            "Freecharge PERSON\n",
            "Unicommerce GPE\n",
            "ShopClues PERSON\n",
            "Indian NORP\n",
            "ShopClues ORG\n",
            "July 2011 DATE\n",
            "Gurugram ORG\n",
            "INR ORG\n",
            "1.1 billion CARDINAL\n",
            "Nexus Venture Partners ORG\n",
            "Helion Ventures ORG\n",
            "more than 5 CARDINAL\n",
            "nine CARDINAL\n",
            "Paytm Mall PERSON\n",
            "Paytm ORG\n",
            "Paytm Mall FAC\n",
            "third ORDINAL\n",
            "Paytm GPE\n",
            "Indian NORP\n",
            "Reliance ORG\n",
            "Reliance ORG\n",
            "India GPE\n",
            "Big Basket ORG\n",
            "two CARDINAL\n",
            "One CARDINAL\n",
            "2013 DATE\n",
            "the last 5 years DATE\n",
            "daily DATE\n",
            "India GPE\n",
            "Indian NORP\n",
            "Digital Mall FAC\n",
            "Asia LOC\n",
            "2020 DATE\n",
            "Digital Mall ORG\n",
            "Asia LOC\n",
            "Yokeasia Malls GPE\n",
            "zero CARDINAL\n",
            "monthly DATE\n",
            "one CARDINAL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F8x83dy30wx"
      },
      "source": [
        "As you saw, spaCy has in-built pipeline ner for Named recogniyion. Though it performs well, it’s not always completely accurate for your text .Sometimes , a word can be categorized as PERSON or a ORG depending upon the context. Also , sometimes the category you want may not be buit-in in spacy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnrkioA-33cA"
      },
      "source": [
        "Observe the above output. Notice that FLIPKART has been identified as PERSON, it should have been ORG . Walmart has also been categorized wrongly as LOC , in this context it should have been ORG . Same goes for Freecharge , ShopClues ,etc..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRVzzUvZ3x2_"
      },
      "source": [
        "# Load pre-existing spacy model\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "# Getting the pipeline component\n",
        "ner=nlp.get_pipe(\"ner\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HY317y-39Ry"
      },
      "source": [
        "spaCy accepts training data as list of tuples.\n",
        "\n",
        "Each tuple should contain the text and a dictionary. The dictionary should hold the start and end indices of the named enity in the text, and the category or label of the named entity.\n",
        "\n",
        "For example, (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})\n",
        "\n",
        "To do this, you’ll need example texts and the character offsets and labels of each entity contained in the texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBBzcxXD386D"
      },
      "source": [
        "# training data\n",
        "TRAIN_DATA = [\n",
        "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
        "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
        "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
        "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
        "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
        "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
        "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
        "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
        "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
        "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
        "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
        "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
        "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
        "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
        "              ]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuG0_VHA4CEJ"
      },
      "source": [
        "The above code clearly shows you the training format. You have to add these labels to the ner using ner.add_label() method of pipeline . Below code demonstrates the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsi-b8id3_UV"
      },
      "source": [
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB3T-ESC4HOK"
      },
      "source": [
        "Now it’s time to train the NER over these examples. But before you train, remember that apart from ner , the model has other pipeline components. These components should not get affected in training.\n",
        "\n",
        "So, disable the other pipeline components through nlp.disable_pipes() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h806w3w4Erz"
      },
      "source": [
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yio1YyEJ4LYc"
      },
      "source": [
        "You have to perform the training with unaffected_pipes disabled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvjM9qSi4N9n"
      },
      "source": [
        "### Training the NER model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFFlxmDV4VHO"
      },
      "source": [
        "First, let’s understand the ideas involved before going to the code.\n",
        "(a) To train an ner model, the model has to be looped over the example for sufficient number of iterations. If you train it for like just 5 or 6 iterations, it may not be effective.\n",
        "\n",
        "(b) Before every iteration it’s a good practice to shuffle the examples randomly throughrandom.shuffle() function .\n",
        "\n",
        "This will ensure the model does not make generalizations based on the order of the examples.\n",
        "\n",
        "(c) The training data is usually passed in batches.\n",
        "\n",
        "You can call the minibatch() function of spaCy over the training data that will return you data in batches . The minibatch function takes size parameter to denote the batch size. You can make use of the utility function compounding to generate an infinite series of compounding values.\n",
        "compunding() function takes three inputs which are start ( the first integer value) ,stop (the maximum value that can be generated) and finally compound. This value stored in compund is the compounding factor for the series.If you are not clear, check out this link for understanding.\n",
        "\n",
        "For each iteration , the model or ner is updated through the nlp.update() command. Parameters of nlp.update() are :\n",
        "\n",
        "docs: This expects a batch of texts as input. You can pass each batch to the zip method, which will return you batches of text and annotations. `\n",
        "golds: You can pass the annotations we got through zip method here\n",
        "\n",
        "drop: This represents the dropout rate.\n",
        "\n",
        "losses: A dictionary to hold the losses against each pipeline component. Create an empty dictionary and pass it here.\n",
        "\n",
        "At each word, the update() it makes a prediction. It then consults the annotations to check if the prediction is right. If it isn’t , it adjusts the weights so that the correct action will score higher next time.\n",
        "\n",
        "Finally, all of the training is done within the context of the nlp model with disabled pipeline, to prevent the other components from being involved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy_POVFq4JSj",
        "outputId": "7a7a249d-7de1-4708-d71f-74e10b76ac93"
      },
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(30):\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "        print(\"Losses\", losses)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses {'ner': 4.972815847482707}\n",
            "Losses {'ner': 7.060777689512179}\n",
            "Losses {'ner': 7.397154785874591}\n",
            "Losses {'ner': 12.679692163284926}\n",
            "Losses {'ner': 15.525220436124073}\n",
            "Losses {'ner': 1.896032368647866}\n",
            "Losses {'ner': 4.921380922939392}\n",
            "Losses {'ner': 7.635120868754569}\n",
            "Losses {'ner': 11.701976171184327}\n",
            "Losses {'ner': 15.463391020928356}\n",
            "Losses {'ner': 2.0279084882036784}\n",
            "Losses {'ner': 3.9485471565654677}\n",
            "Losses {'ner': 3.955853801391271}\n",
            "Losses {'ner': 7.297129265698089}\n",
            "Losses {'ner': 13.672626542068969}\n",
            "Losses {'ner': 6.45540950720207}\n",
            "Losses {'ner': 10.455944284516967}\n",
            "Losses {'ner': 12.8789729087433}\n",
            "Losses {'ner': 14.87061946092522}\n",
            "Losses {'ner': 18.268121289103284}\n",
            "Losses {'ner': 4.522875945782289}\n",
            "Losses {'ner': 5.918226479333725}\n",
            "Losses {'ner': 9.883843705744141}\n",
            "Losses {'ner': 11.329770056516054}\n",
            "Losses {'ner': 11.33256825987496}\n",
            "Losses {'ner': 3.57964714434587}\n",
            "Losses {'ner': 3.6586533139629296}\n",
            "Losses {'ner': 3.6934490205757697}\n",
            "Losses {'ner': 3.762100345063061}\n",
            "Losses {'ner': 8.793862813458816}\n",
            "Losses {'ner': 0.01676330766065348}\n",
            "Losses {'ner': 0.046285999994211124}\n",
            "Losses {'ner': 0.7238443149589102}\n",
            "Losses {'ner': 1.7093724319894363}\n",
            "Losses {'ner': 4.450092186760571}\n",
            "Losses {'ner': 0.809399011768619}\n",
            "Losses {'ner': 6.62807031474199}\n",
            "Losses {'ner': 7.579191116308262}\n",
            "Losses {'ner': 9.694560566564828}\n",
            "Losses {'ner': 9.694597622345096}\n",
            "Losses {'ner': 0.015590285293512807}\n",
            "Losses {'ner': 0.04909972859655909}\n",
            "Losses {'ner': 4.387318513650428}\n",
            "Losses {'ner': 4.464357830614119}\n",
            "Losses {'ner': 6.056924700240359}\n",
            "Losses {'ner': 1.997317303699674}\n",
            "Losses {'ner': 3.997864711114744}\n",
            "Losses {'ner': 4.926828259202466}\n",
            "Losses {'ner': 6.961338704468744}\n",
            "Losses {'ner': 6.961564064702188}\n",
            "Losses {'ner': 0.0019564398594162924}\n",
            "Losses {'ner': 0.4622550341824514}\n",
            "Losses {'ner': 1.2074367975007796}\n",
            "Losses {'ner': 1.209034974053342}\n",
            "Losses {'ner': 4.009866409720299}\n",
            "Losses {'ner': 2.0126772946096025}\n",
            "Losses {'ner': 3.3400192231893016}\n",
            "Losses {'ner': 3.363492566010329}\n",
            "Losses {'ner': 3.3680972431989673}\n",
            "Losses {'ner': 7.2178557135173875}\n",
            "Losses {'ner': 2.2188655674997335}\n",
            "Losses {'ner': 2.219788571921482}\n",
            "Losses {'ner': 2.21990232739688}\n",
            "Losses {'ner': 5.163299660823725}\n",
            "Losses {'ner': 5.167822641727554}\n",
            "Losses {'ner': 0.0102637889935977}\n",
            "Losses {'ner': 0.027510212062338724}\n",
            "Losses {'ner': 1.4945104913905425}\n",
            "Losses {'ner': 2.1980313313744233}\n",
            "Losses {'ner': 2.987241944799763}\n",
            "Losses {'ner': 0.000707989599220582}\n",
            "Losses {'ner': 0.13678689835072078}\n",
            "Losses {'ner': 1.6925607774253848}\n",
            "Losses {'ner': 2.883223175131649}\n",
            "Losses {'ner': 2.8836097871457134}\n",
            "Losses {'ner': 1.1372937424394634}\n",
            "Losses {'ner': 2.2551980414303783}\n",
            "Losses {'ner': 4.260850783417157}\n",
            "Losses {'ner': 5.960121683052097}\n",
            "Losses {'ner': 7.561086430492978}\n",
            "Losses {'ner': 0.07697879733586888}\n",
            "Losses {'ner': 0.21766705654600438}\n",
            "Losses {'ner': 0.2595497888751752}\n",
            "Losses {'ner': 0.2606327644495807}\n",
            "Losses {'ner': 1.7329831567313545}\n",
            "Losses {'ner': 0.0007821148436737285}\n",
            "Losses {'ner': 0.016932087079970577}\n",
            "Losses {'ner': 0.11871818032887309}\n",
            "Losses {'ner': 0.8787189925933525}\n",
            "Losses {'ner': 2.6917253323495416}\n",
            "Losses {'ner': 0.4074292667316377}\n",
            "Losses {'ner': 0.40745435397351093}\n",
            "Losses {'ner': 0.40795432093495965}\n",
            "Losses {'ner': 0.5335900873341375}\n",
            "Losses {'ner': 2.035597980102592}\n",
            "Losses {'ner': 0.4682668361294353}\n",
            "Losses {'ner': 0.4683881689232831}\n",
            "Losses {'ner': 0.46839849595295946}\n",
            "Losses {'ner': 0.5654674308622789}\n",
            "Losses {'ner': 0.565535809110488}\n",
            "Losses {'ner': 1.2983745665844326}\n",
            "Losses {'ner': 2.518866989422844}\n",
            "Losses {'ner': 3.2527194825933217}\n",
            "Losses {'ner': 3.2531844837293438}\n",
            "Losses {'ner': 3.2577541958782037}\n",
            "Losses {'ner': 1.7854448551043944}\n",
            "Losses {'ner': 1.7880859235931548}\n",
            "Losses {'ner': 2.461577584180606}\n",
            "Losses {'ner': 2.4624713717158127}\n",
            "Losses {'ner': 2.462799462598589}\n",
            "Losses {'ner': 0.001270676485895593}\n",
            "Losses {'ner': 1.791213810171616}\n",
            "Losses {'ner': 3.691559908238265}\n",
            "Losses {'ner': 4.598037870053794}\n",
            "Losses {'ner': 4.59803832552348}\n",
            "Losses {'ner': 1.4949331552618905e-05}\n",
            "Losses {'ner': 0.14565536553278674}\n",
            "Losses {'ner': 0.14647342028181126}\n",
            "Losses {'ner': 0.14647664038988212}\n",
            "Losses {'ner': 0.409383558386837}\n",
            "Losses {'ner': 4.1547535079899944e-05}\n",
            "Losses {'ner': 0.38167489067478333}\n",
            "Losses {'ner': 0.3816764120677788}\n",
            "Losses {'ner': 0.3816784094146789}\n",
            "Losses {'ner': 0.3963150813241101}\n",
            "Losses {'ner': 1.9693600029104632}\n",
            "Losses {'ner': 2.1360974988060155}\n",
            "Losses {'ner': 2.5338184443360054}\n",
            "Losses {'ner': 2.5338433701005068}\n",
            "Losses {'ner': 2.533850728693838}\n",
            "Losses {'ner': 0.001587572755425981}\n",
            "Losses {'ner': 0.0016775639887013777}\n",
            "Losses {'ner': 0.009571137896121615}\n",
            "Losses {'ner': 0.009571150322348666}\n",
            "Losses {'ner': 0.00958998811992225}\n",
            "Losses {'ner': 0.05337947016051803}\n",
            "Losses {'ner': 0.05338747360879675}\n",
            "Losses {'ner': 0.05339770762486178}\n",
            "Losses {'ner': 0.05504404452388751}\n",
            "Losses {'ner': 0.05505595003086105}\n",
            "Losses {'ner': 3.983025710473792e-05}\n",
            "Losses {'ner': 0.0002320093612941873}\n",
            "Losses {'ner': 0.00031388775319249416}\n",
            "Losses {'ner': 0.4551166271713303}\n",
            "Losses {'ner': 2.428758400015804}\n",
            "Losses {'ner': 5.02455794017915e-06}\n",
            "Losses {'ner': 6.960781083055793e-05}\n",
            "Losses {'ner': 7.021713286474755e-05}\n",
            "Losses {'ner': 0.01945296916332267}\n",
            "Losses {'ner': 0.021700909550553957}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "kiV2tIV84Xrd",
        "outputId": "30784c9b-354c-44c6-a49a-9acfc0587da3"
      },
      "source": [
        "# Testing the model\n",
        "doc = nlp(\"I was driving a Alto\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-241f4248a740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Testing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I was driving a Alto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entities\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Pipe.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Pipe.require_model\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E109] Model for component 'textcat' not initialized. Did you forget to load a model, or forget to call begin_training()?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZd0jH4p46fQ"
      },
      "source": [
        "You can observe that even though I didn’t directly train the model to recognize “Alto” as a vehicle name, it has predicted based on the similarity of context.\n",
        "This is the awesome part of the NER model.\n",
        "\n",
        "The model does not just memorize the training examples. It should learn from them and be able to generalize it to new examples.\n",
        "\n",
        "Once you find the performance of the model satisfactory, save the updated model.\n",
        "\n",
        "You can save it your desired directory through the to_disk command.\n",
        "\n",
        "After saving, you can load the model from the directory at any point of time by passing the directory path to spacy.load() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m5E9riL41l-"
      },
      "source": [
        "# Save the  model to directory\n",
        "output_dir = Path('/content/')\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)\n",
        "\n",
        "# Load the saved model and predict\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp_updated = spacy.load(output_dir)\n",
        "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}